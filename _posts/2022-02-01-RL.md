---
title: "INTRODUCTION À L’APPRENTISSAGE PAR RENFORCEMENT"
categories:
  - RL
tags:
  - Apprentissage par renforcement en français
  - Reinforcement learning en français
  - RL en français
  - RL French
excerpt : "RL – Introduction à l’apprentissage par renforcement"
header :
    overlay_image: "https://github.com/lbourdois/blog/blob/master/assets/images/NLP_radom_blog.png"
    teaser : "https://github.com/lbourdois/blog/blob/master/assets/images/RL/RL_illustration.png"
author_profile: false
sidebar:
    nav: sidebar-sample
classes: wide
---

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


# <span style="color: #FF0000"> **Avant-propos** </span>
Cet article la traduction de l’article de Lilian Weng : [A (Long) Peek into Reinforcement Learning](https://lilianweng.github.io/lil-log/2018/02/19/a-long-peek-into-reinforcement-learning.html). Merci à elle de m’avoir autorisé à effectuer cette traduction. J’ai ajouté des éléments supplémentaires quand j’estimais que cela était pertinent.
<br><br><br>

# <span style="color: #FF0000"> **Introduction** </span>
Ces dernières années, AlphaGo a battu le meilleur joueur humain professionnel au jeu de Go. Très vite, l'algorithme AlphaGo Zero a battu AlphaGo par 100-0 sans apprentissage supervisé sur des connaissances humaines. Les meilleurs joueurs professionnels ont perdu contre le robot développé par OpenAI dans la compétition DOTA2 1v1. Après avoir pris connaissance de ces faits, il est assez difficile de ne pas être curieux de connaître la magie qui se cache derrière ces algorithmes : l'apprentissage par renforcement (RL pour *Reinforcement Learning* en anglais). Cet article est un bref tour d'horizon de ce domaine. Nous présenterons d'abord plusieurs concepts fondamentaux, puis nous nous plongerons dans les approches classiques de résolution des problèmes d'apprentissage par renforcement. 
<br><br><br>

#  <span style="color: #FF0000"> **Qu'est-ce que l'apprentissage par renforcement ?** </span>
Supposons que nous ayons un agent situé dans un environnement inconnu et que cet agent puisse obtenir des récompenses en interagissant avec l'environnement. L'agent doit entreprendre des actions de manière à maximiser les récompenses cumulées. Dans la réalité, le scénario peut être par exemple celui d'un robot jouant à un jeu dans le but d’obtenir un score élevé, ou d'un robot essayant d'accomplir des tâches physiques avec des objets.


![Illustration d'un problème d'apprentissage par renforcement]({{ '/assets/images/RL/RL_illustration.png' | relative_url }})
{ : style="width : 70% ;" class="center"}
*Un agent interagit avec l'environnement, en essayant de prendre des mesures intelligentes pour maximiser les récompenses cumulatives.*


L'objectif de l'apprentissage par renforcement est d'apprendre une bonne stratégie pour l'agent à partir d'essais expérimentaux et d'un retour d'information relativement simple. Avec la stratégie optimale, l'agent est capable de s'adapter activement à l'environnement pour maximiser les récompenses futures.


#### Concepts clés

Maintenant, définissons formellement un ensemble de concepts clés en RL.

L'agent agit dans un **environnement**. La façon dont l'environnement réagit à certaines actions est définie par un **modèle** que nous pouvons ou non connaître. L'agent peut rester dans l'un des nombreux **états** ($$s \in \mathcal{S}$$ (*s* pour *stats* en anglais) de l'environnement, et choisir de prendre l'une des nombreuses **actions** ($$a \in \mathcal{A}$$) pour passer d'un état à un autre. L'état dans lequel l'agent arrivera est décidé par les probabilités de transition entre les états ($$P$$). Lorsqu'une action est entreprise, l'environnement délivre une **récompense** ($$r \in \mathcal{R}$$) en retour. 

Le modèle définit la fonction de récompense et les probabilités de transition. Nous pouvons ou non connaître comment le modèle fonctionne :
- **Nous connaissons le modèle** : planification avec des informations parfaites. Lorsque nous connaissons parfaitement l'environnement, nous pouvons trouver la solution optimale par [programmation dynamique] (https://fr.wikipedia.org/wiki/Programmation_dynamique). 
- **Nous ne connaissons pas le modèle** : apprentissage avec des informations incomplètes. Nous faisons du RL sans modèle ou essayons d'apprendre le modèle explicitement dans le cadre de l'algorithme. La plupart du contenu suivant sert les scénarios où le modèle est inconnu.

La **politique** $$\pi(s)$$ de l'agent fournit la ligne directrice sur quelle est l'action optimale à prendre dans un certain état. <span style="color : #e01f1f ;">**Le but étant de maximiser les récompenses totales**</span>. Chaque état est associé à une **fonction de valeur** $$V(s)$$ prédisant le montant attendu des récompenses futures que nous sommes capables de recevoir dans cet état en agissant selon la politique correspondante. En d'autres termes, la fonction de valeur quantifie la qualité d'un état. Les fonctions de politique et de valeur sont ce que nous essayons d'apprendre dans l'apprentissage par renforcement.


![Catégorisation des algorithmes RL]({{ '/assets/images/RL_algorithme_catégorisation.png' | relative_url }})
{ : style="width : 100% ;" class="center"}
*Résumé des approches en RL selon que l'on veut modéliser la valeur, la politique ou l'environnement. (Source de l'image : reproduction issue du [cours 1](https://youtu.be/2pWv7GOvuf0de) de David Silver sur le RL).)*


L'interaction entre l'agent et l'environnement implique une séquence d'actions et de récompenses observées dans le temps, $$t=1, 2, \dots, T$$. Au cours de ce processus, l'agent accumule de la connaissance sur l'environnement, apprend la politique optimale et prend des décisions sur l'action suivante afin d'apprendre efficacement la meilleure politique. Appelons respectivement l'état, l'action et la récompense au pas de temps t : $$S_t$$, $$A_t$$, et $$R_t$$. Ainsi, la séquence d'interaction est entièrement décrite par un **épisode** (également appelé « essai » ou « trajectoire ») et la séquence se termine à l'état terminal $$S_T$$ :

$$
S_1, A_1, R_2, S_2, A_2, \dots, S_T
$$


Quelques termes que vous rencontrerez souvent lorsque vous vous plongerez dans les différentes catégories d'algorithmes d’apprentissage par renforcement :
- **Basé sur un modèle** (*model-based*) : s'appuie sur le modèle de l'environnement. Soit le modèle est connu, soit l'algorithme l'apprend explicitement.
- **Sans modèle** (*Model-free*) : aucune dépendance vis-à-vis du modèle pendant l'apprentissage.
- **Sur la politique** (*Off-policy*) : utilisations des résultats déterministes ou les échantillons de la politique cible afin d’entraîner l'algorithme.
- **Off-policy** (*Off-policy*) : entraînement sur une distribution de transitions ou d'épisodes produite par une politique de comportement différente de celle produite par la politique cible.
