---
title: "INTRODUCTION A L’APPRENTISSAGE PAR RENFORCEMENT"
categories:
  - RL
tags:
  - Apprentissage par renforcement en français
  - Reinforcement learning en français
  - RL en français
  - RL French
excerpt : "RL – Introduction à l’apprentissage par renforcement"
header :
    overlay_image: "https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/Transformer/Transformers_blog.png"
    teaser : "https://raw.githubusercontent.com/lbourdois/blog/master/assets/images/Transformer/transformer_resideual_layer_norm_3.png"
author_profile: false
sidebar:
    nav: sidebar-sample
classes: wide
---

<script type="text/javascript" async
  src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


# <span style="color: #FF0000"> **Avant-propos** </span>

> Dans ce billet, nous allons passer brièvement en revue le domaine de l'apprentissage par renforcement (RL), des concepts fondamentaux aux algorithmes classiques. Nous espérons que cet examen sera suffisamment utile pour que les débutants ne se perdent pas dans les termes spécialisés et le jargon au moment de commencer.



Quelques nouvelles passionnantes dans le domaine de l'intelligence artificielle (IA) viennent de se produire ces dernières années.  AlphaGo a battu le meilleur joueur humain professionnel au jeu de Go. Très vite, l'algorithme étendu AlphaGo Zero a battu AlphaGo par 100-0 sans apprentissage supervisé sur les connaissances humaines. Les meilleurs joueurs professionnels ont perdu contre le robot développé par OpenAI dans la compétition DOTA2 1v1. Après avoir pris connaissance de ces faits, il est assez difficile de ne pas être curieux de connaître la magie qui se cache derrière ces algorithmes --- l'apprentissage par renforcement (RL). J'écris ce billet pour faire un bref tour d'horizon de ce domaine. Nous présenterons d'abord plusieurs concepts fondamentaux, puis nous nous plongerons dans les approches classiques de résolution des problèmes d'apprentissage par renforcement. J'espère que ce billet sera un bon point de départ pour les débutants, et qu'il permettra de faire le lien entre l'étude future et la recherche de pointe.

# <span style="color: #FF0000"> **Qu'est-ce que l'apprentissage par renforcement ?** </span>

Supposons que nous ayons un agent dans un environnement inconnu et que cet agent puisse obtenir certaines récompenses en interagissant avec l'environnement. L'agent doit entreprendre des actions de manière à maximiser les récompenses cumulées. Dans la réalité, le scénario pourrait être celui d'un robot jouant à un jeu pour obtenir des scores élevés, ou d'un robot essayant d'accomplir des tâches physiques avec des objets physiques, et pas seulement dans ces cas-là.


![Illustration d'un problème d'apprentissage par renforcement]({{ '/assets/images/RL_illustration.png' | relative_url }})
{ : style="width : 70% ;" class="center"}
*Un agent interagit avec l'environnement, en essayant de prendre des mesures intelligentes pour maximiser les récompenses cumulatives.*


L'objectif de l'apprentissage par renforcement (RL) est d'apprendre une bonne stratégie pour l'agent à partir d'essais expérimentaux et d'un retour d'information relativement simple. Avec la stratégie optimale, l'agent est capable de s'adapter activement à l'environnement pour maximiser les récompenses futures.


#### Concepts clés

Maintenant, définissons formellement un ensemble de concepts clés en RL.

L'agent agit dans un **environnement**. La façon dont l'environnement réagit à certaines actions est définie par un **modèle** que nous pouvons ou non connaître. L'agent peut rester dans l'un des nombreux **états** ($$s \in \mathcal{S}$$) de l'environnement, et choisir de prendre l'une des nombreuses **actions** ($$a \in \mathcal{A}$$) pour passer d'un état à un autre. L'état dans lequel l'agent arrivera est décidé par les probabilités de transition entre les états ($$P$$). Lorsqu'une action est entreprise, l'environnement délivre une **récompense** ($$r \in \mathcal{R}$$) en retour. 

Le modèle définit la fonction de récompense et les probabilités de transition. Nous pouvons ou non savoir comment le modèle fonctionne et cela différencie deux circonstances :
- **Connaître le modèle** : planification avec des informations parfaites ; faire de la RL basée sur le modèle. Lorsque nous connaissons parfaitement l'environnement, nous pouvons trouver la solution optimale par [programmation dynamique] (https://en.wikipedia.org/wiki/Dynamic_programming) (DP). Vous vous souvenez encore de la "sous-séquence croissante la plus longue" ou du "problème du voyageur de commerce" de votre cours d'algorithmique 101 ? LOL. Mais ce n'est pas le sujet de cet article. 
- **Ne connaît pas le modèle** : apprentissage avec des informations incomplètes ; faire du RL sans modèle ou essayer d'apprendre le modèle explicitement dans le cadre de l'algorithme. La plupart du contenu suivant sert les scénarios où le modèle est inconnu.

La **politique** $$\pi(s)$$ de l'agent fournit la ligne directrice sur quelle est l'action optimale à prendre dans un certain état avec <span style="color : #e01f1f ;">**le but étant de maximiser les récompenses totales**</span>. Chaque état est associé à une **fonction de valeur** $$V(s)$$ prédisant le montant attendu des récompenses futures que nous sommes capables de recevoir dans cet état en agissant selon la politique correspondante. En d'autres termes, la fonction de valeur quantifie la qualité d'un état. Les fonctions de politique et de valeur sont toutes deux ce que nous essayons d'apprendre dans l'apprentissage par renforcement.


![Catégorisation des algorithmes RL]({{ '/assets/images/RL_algorithme_catégorisation.png' | relative_url }})
{ : style="width : 100% ;" class="center"}
*Fig. 2. Résumé des approches en RL selon que l'on veut modéliser la valeur, la politique ou l'environnement. (Source de l'image : reproduite du cours de David Silver sur la RL [cours 1] (https://youtu.be/2pWv7GOvuf0).)*


L'interaction entre l'agent et l'environnement implique une séquence d'actions et de récompenses observées dans le temps, $$t=1, 2, \dots, T$$. Au cours de ce processus, l'agent accumule les connaissances sur l'environnement, apprend la politique optimale et prend des décisions sur l'action suivante afin d'apprendre efficacement la meilleure politique. Appelons l'état, l'action et la récompense au pas de temps t $$S_t$$, $$A_t$$, et $$R_t$$, respectivement. Ainsi, la séquence d'interaction est entièrement décrite par un **épisode** (également appelé "essai" ou "trajectoire") et la séquence se termine à l'état terminal $$S_T$$ :

$$
S_1, A_1, R_2, S_2, A_2, \dots, S_T
$$


Des termes que vous rencontrerez souvent lorsque vous vous plongerez dans les différentes catégories d'algorithmes RL :
- **Base sur un modèle** : S'appuie sur le modèle de l'environnement ; soit le modèle est connu, soit l'algorithme l'apprend explicitement.
- **Sans modèle** : Aucune dépendance vis-à-vis du modèle pendant l'apprentissage.
- **Sur la politique** : Utilisez les résultats déterministes ou les échantillons de la politique cible pour entraîner l'algorithme.
- **Off-policy** : Formation sur une distribution de transitions ou d'épisodes produite par une politique de comportement différente de celle produite par la politique cible.


#### Modèle : Transition et récompense

Le modèle est un descripteur de l'environnement. Grâce au modèle, nous pouvons apprendre ou déduire comment l'environnement interagit avec l'agent et lui fournit des informations en retour. Le modèle comporte deux parties principales, la fonction de probabilité de transition $$P$$ et la fonction de récompense $$R$$.

Disons que lorsque nous sommes dans l'état s, nous décidons de faire une action a pour arriver dans l'état suivant s' et obtenir la récompense r. Ceci est connu comme une étape de **transition**, représentée par un tuple (s, a, s', r).

La fonction de transition P enregistre la probabilité de passer de l'état s à s' après avoir effectué l'action a tout en obtenant la récompense r. Nous utilisons $$\mathbb{P}$$ comme symbole de "probabilité".

$$
P(s', r \vert s, a) = \mathbb{P} [S_{t+1} = s', R_{t+1} = r \vert S_t = s, A_t = a]
$$

Ainsi, la fonction de transition d'état peut être définie comme une fonction de $$P(s', r \vert s, a)$$ :

$$
P_{ss}^a' = P(s' \vert s, a) = \mathbb{P} [S_{t+1} = s' \vert S_t = s, A_t = a] = \sum_{r \in \mathcal{R}} P(s', r \vert s, a)
$$

La fonction de récompense R prédit la prochaine récompense déclenchée par une action :

$$
R(s, a) = \mathbb{E} [R_{t+1} \vert S_t = s, A_t = a] = \sum_{r\in\mathcal{R}} r \sum_{s' \in \mathcal{S}} P(s', r \vert s, a)
$$


#### Politique

La politique, en tant que fonction de comportement de l'agent $$\pi$$, nous indique quelle action entreprendre dans l'état s. Il s'agit d'une correspondance entre l'état s et l'action a et elle peut être déterministe ou stochastique :
- Déterministe : $$\pi(s) = a$$.
- Stochastique : $$\pi(a \vert s) = \mathbb{P}_\pi [A=a \vert S=s]$$.


#### Fonction de la valeur

La fonction de valeur mesure la qualité d'un état ou le degré de récompense d'un état ou d'une action par une prédiction de la récompense future. La récompense future, également appelée **rendement**, est la somme totale des récompenses actualisées à venir. Calculons le rendement $$G_t$$ à partir du temps t :

$$
G_t = R_{t+1} + \gamma R_{t+2} + \dots = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

Le facteur d'actualisation $$\gamma \in [0, 1]$$ pénalise les récompenses dans le futur, car :
- Les récompenses futures peuvent présenter une plus grande incertitude ; c'est le cas du marché boursier.
- Les récompenses futures n'apportent pas de bénéfices immédiats ; c'est-à-dire qu'en tant qu'êtres humains, nous préférons nous amuser aujourd'hui plutôt que 5 ans plus tard ;).
- L'actualisation offre une commodité mathématique, c'est-à-dire que nous n'avons pas besoin de suivre indéfiniment les étapes futures pour calculer le rendement.
- Nous n'avons pas besoin de nous soucier des boucles infinies dans le graphe de transition d'état.

La **valeur d'état** d'un état s est le rendement attendu si nous sommes dans cet état au temps t, $$S_t = s$$ :

$$
V_{\pi}(s) = \mathbb{E}_{\pi}[G_t \vert S_t = s]
$$

De même, nous définissons la **valeur d'action** ("valeur-Q" ; Q comme "qualité", je crois ?) d'une paire état-action comme suit :

$$
Q_{\pi}(s, a) = \mathbb{E}_{\pi}[G_t \vert S_t = s, A_t = a]
$$

De plus, puisque nous suivons la politique cible $$\pi$$, nous pouvons utiliser la distribution de probabilité sur les actions possibles et les valeurs Q pour récupérer la valeur d'état :

$$
V_{\pi}(s) = \sum_{a \in \mathcal{A}} Q_{\pi}(s, a) \pi(a \vert s)
$$

La différence entre la valeur-action et la valeur-état est la fonction d'action **avantage** ("valeur-A") :

$$
A_{\pi}(s, a) = Q_{\pi}(s, a) - V_{\pi}(s)
$$


#### Valeur optimale et politique

La fonction de valeur optimale produit le rendement maximal :

$$
V_{*}(s) = \max_{\pi} V_{\pi}(s),
Q_{*}(s, a) = \max_{\pi} Q_{\pi}(s, a)
$$

La politique optimale permet d'obtenir des fonctions de valeur optimales :

$$
\pi_{*} = \arg\max_{*pi} V_{*pi}(s),
\pi_{*} = \arg\max_{\pi} Q_{\pi}(s, a)
$$

Et bien sûr, nous avons $$V_{\pi_{*}}(s)=V_{*}(s)$$ et $$Q_{\pi_{*}(s, a) = Q_{*}(s, a)$$.


### Processus de décision de Markov

En termes plus formels, presque tous les problèmes de RL peuvent être formulés comme des **processus de décision de Markov** (MDP). Tous les états dans les MDP ont la propriété "Markov", faisant référence au fait que le futur ne dépend que de l'état actuel, et non de l'historique :

$$
\mathbb{P}[ S_{t+1} \vert S_t ] = \mathbb{P} [S_{t+1} \vert S_1, \dots, S_t]
$$

Ou, en d'autres termes, le futur et le passé sont **conditionnellement indépendants** étant donné le présent, car l'état actuel englobe toutes les statistiques dont nous avons besoin pour décider du futur.


![Interaction agent-environnement dans MDP]({{ '/assets/images/agent_environment_MDP.png' | relative_url }})
{ : style="width : 60% ;" class="center"}
*Fig. 3. L'interaction agent-environnement dans un processus de décision de Markov. (Source de l'image : Sec. 3.1 Sutton & Barto (2017).)*


Un processus de déicision de Markov est constitué de cinq éléments $$\mathcal{M} = \langle \mathcal{S}, \mathcal{A}, P, R, \gamma \rangle$$, où les symboles ont la même signification que les concepts clés de la section [précédente](#concepts clés), bien alignés sur les paramètres du problème RL :
- $$\mathcal{S}$$ - un ensemble d'états ;
- $$\mathcal{A}$$ - un ensemble d'actions ;
- $$P$$ - fonction de probabilité de transition ;
- $$R$$ - fonction de récompense ;
- $$\gamma$$ - facteur d'actualisation des récompenses futures.
Dans un environnement inconnu, nous n'avons pas une connaissance parfaite de $$P$$ et $$R$$.


![Exemple de PDM]({{ '/assets/images/mdp_example.jpg' | relative_url }})
{ : class="center"}
*Fig. 4. Un exemple amusant de processus de décision de Markov : une journée de travail typique. (Source de l'image : [randomant.net/reinforcement-learning-concepts](https://randomant.net/reinforcement-learning-concepts/))*


### Équations de Bellman

Les équations de Bellman font référence à un ensemble d'équations qui décomposent la fonction de valeur en la récompense immédiate plus les valeurs futures actualisées.

$$
\begin{aligned}
V(s) &= \mathbb{E} [G_t \vert S_t = s] \\\N
&= \mathbb{E} [R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \vert S_t = s] \\N
&= \mathbb{E} [R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \vert S_t = s] \\\N
&= \mathbb{E} [R_{t+1} + \gamma G_{t+1} \vert S_t = s] \\N
&= \mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) \vert S_t = s]
\end{aligned}
$$

De même pour la valeur Q,

$$
\begin{aligned}
Q(s, a) 
&= \mathbb{E} [R_{t+1} + \gamma V(S_{t+1}) \mid S_t = s, A_t = a] \{\i}
&= \mathbb{E} [R_{t+1} + \gamma \mathbb{E}_{a\sim\pi} Q(S_{t+1}, a) \mid S_t = s, A_t = a]
\end{aligned}
$$


#### Équations d'espérance de Bellman

Le processus de mise à jour récursif peut être décomposé en équations construites sur des fonctions état-valeur et action-valeur. Au fur et à mesure que nous avançons dans les étapes d'action futures, nous étendons V et Q alternativement en suivant la politique $$\pi$$.


![Bellman]({{ '/assets/images/bellman_equation.png' | relative_url }})
{ : style="width : 60% ;" class="center"}
*Fig. 5. Illustration de la façon dont les équations d'expection de Bellman mettent à jour les fonctions état-valeur et action-valeur*.


$$
\begin{aligned}
V_{\pi}(s) &= \sum_{a \in \mathcal{A}} \pi(a \vert s) Q_{\pi}(s, a) \\\\{\c}
Q_{\pi}(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_{\pi} (s') \\N
V_{\pi}(s) &= \sum_{a \in \mathcal{A}} \pi(a \vert s) \big( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_{\pi} (s') \big) \\N{\c}
Q_{\pi}(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \sum_{a' \in \mathcal{A}} \pi(a' \vert s') Q_{\pi} (s', a')
\end{aligned}
$$


#### Équations d'optimalité de Bellman

Si nous ne sommes intéressés que par les valeurs optimales, plutôt que de calculer l'espérance en suivant une politique, nous pourrions passer directement aux rendements maximums pendant les mises à jour alternatives sans utiliser de politique. RECAP : les valeurs optimales $$V_*$$ et $$Q_*$$ sont les meilleurs rendements que nous pouvons obtenir, définis [ici](#optimal-value-and-policy).

$$
\begin{aligned}
V_*(s) &= \max_{a \in \mathcal{A}} Q_*(s,a)\\\
Q_*(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_*(s') \\N
V_*(s) &= \max_{a \in \mathcal{A}} \big( R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a V_*(s') \big) \\\\N
Q_*(s, a) &= R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P_{ss'}^a \max_{a' \in \mathcal{A}} Q_*(s', a')
\end{aligned}
$$

Sans surprise, elles ressemblent beaucoup aux équations d'espérance de Bellman.

Si nous disposons d'une information complète sur l'environnement, cela se transforme en un problème de planification, soluble par DP. Malheureusement, dans la plupart des scénarios, nous ne connaissons pas $$P_{ss}^a$$' ou $$R(s, a)$$, nous ne pouvons donc pas résoudre les PDM en appliquant directement les équations de Bellmen, mais cela pose les bases théoriques de nombreux algorithmes de RL.



## Approches communes

Il est maintenant temps de passer en revue les principales approches et les algorithmes classiques pour résoudre les problèmes de RL. Dans les prochains billets, je prévois d'approfondir chaque approche.


### Programmation dynamique

Lorsque le modèle est entièrement connu, suivant les équations de Bellman, nous pouvons utiliser la [programmation dynamique] (https://en.wikipedia.org/wiki/Dynamic_programming) (DP) pour évaluer itérativement les fonctions de valeur et améliorer la politique.


#### Évaluation de la politique

L'évaluation de la politique consiste à calculer la valeur d'état $$V_\pi$$ pour une politique $$\pi$$ donnée :

$$
V_{t+1}(s) 
= \mathbb{E}_\pi [r + \gamma V_t(s') | S_t = s]
= \sum_a \pi(a \vert s) \sum_{s', r} P(s', r \vert s, a) (r + \gamma V_t(s'))
$$

#### Amélioration de la politique

Sur la base des fonctions de valeur, l'amélioration de la politique génère une meilleure politique $$\pi' \geq \pi$$ en agissant avec avidité.

$$
Q_\pi(s, a) 
= \mathbb{E} [R_{t+1} + \gamma V_\pi(S_{t+1}) \vert S_t=s, A_t=a]
= \sum_{s', r} P(s', r \vert s, a) (r + \gamma V_\pi(s'))
$$

#### Itération de la politique

L'algorithme *Generalized Policy Iteration (GPI)* fait référence à une procédure itérative pour améliorer la politique lorsqu'on combine évaluation et amélioration de la politique.

$$
\pi_0 \xrightarrow[]{\text{evaluation}} V_{\pi_0} \xrightarrow[]{\text{improve}}
\pi_1 \xrightarrow[]{\text{evaluation}} V_{\pi_1} \xrightarrow[]{\text{improve}}
\pi_2 \xrightarrow[]{\text{evaluation}} \dots \xrightarrow[]{\text{improve}}
\pi_* \xrightarrow[]{\text{evaluation}} V_*
$$

Dans GPI, la fonction de valeur est approchée de manière répétée pour se rapprocher de la valeur réelle de la politique actuelle et, dans le même temps, la politique est améliorée de manière répétée pour se rapprocher de l'optimalité. Ce processus d'itération de la politique fonctionne et converge toujours vers l'optimalité, mais pourquoi est-ce le cas ?

Disons que nous avons une politique $$\pi$$ et que nous générons ensuite une version améliorée $$\pi'$$ en prenant des actions avec avidité, $$\pi'(s) = \arg\max_{a \in \mathcal{A}} Q_\pi(s, a)$$. La valeur de ce $$\pi'$$ amélioré est garantie meilleure car :

$$
\begin{aligned}
Q_\pi(s, \pi'(s))
&= Q_\pi(s, \arg\max_{a \in \mathcal{A}} Q_\pi(s, a)) \\
&= \max_{a \in \mathcal{A}} Q_\pi(s, a) \geq Q_\pi(s, \pi(s)) = V_\pi(s)
\end{aligned}
$$


### Méthodes de Monte-Carlo

Tout d'abord, rappelons que $$V(s) = \mathbb{E}[ G_t \vert S_t=s]$$. Les méthodes de Monte-Carlo (MC) utilisent une idée simple : elles apprennent à partir d'épisodes d'expérience brute sans modéliser la dynamique de l'environnement et calculent le rendement moyen observé comme une approximation du rendement attendu. Pour calculer le rendement empirique $$G_t$$, les méthodes MC doivent apprendre de <span style="color : #e01f1f ;">**complet**</span> épisodes $$S_1, A_1, R_2, \dots, S_T$$ pour calculer $$G_t = \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1}$$ et tous les épisodes doivent finalement se terminer.

Le rendement moyen empirique pour l'état s est :

$$
V(s) = \frac{\i1}{\i1}Sum_{t=1}^T \mathbb{1}[S_t = s] G_t}{\i1}Sum_{t=1}^T \mathbb{1}[S_t = s]}
$$

où $$\mathbb{1}[S_t = s]$$ est une fonction indicatrice binaire. Nous pouvons compter la visite de l'état s à chaque fois, de sorte qu'il peut y avoir plusieurs visites d'un état dans un épisode ("every-visit"), ou ne la compter que la première fois que nous rencontrons un état dans un épisode ("first-visit"). Cette méthode d'approximation peut être facilement étendue aux fonctions action-valeur en comptant la paire (s, a).

$$
Q(s, a) = \frac{\i1} \mathbb{\i1}^T \mathbb{\i1}[S_t = s, A_t = a] G_t}{\i1}^T \mathbb{\i1}[S_t = s, A_t = a]}
$$

Pour apprendre la politique optimale par MC, nous l'itérons en suivant une idée similaire à [GPI](#policy-iteration).

![Itération de la politique par MC]({{ '/assets/images/MC_control.png' | relative_url }})
{ : style="width : 50% ;" class="center"}

1. Améliorer la politique de manière avide par rapport à la fonction de valeur actuelle : $$\pi(s) = \arg\max_{a \in \mathcal{A}} Q(s, a)$$.
2. Générer un nouvel épisode avec la nouvelle politique $$\pi$$ (c'est-à-dire que l'utilisation d'algorithmes comme [ε-greedy]({{ site.baseurl }}{% post_url 2018-01-23-the-multi-armed-bandit-problem-and-its-solutions %}#%CE%B5-greedy-algorithm) nous aide à trouver un équilibre entre exploitation et exploration).
3. Estimez Q en utilisant le nouvel épisode : $$q_\pi(s, a) = \frac{\sum_{t=1}^T \big( \mathbb{1}[S_t = s, A_t = a] \sum_{k=0}^{T-t-1} \gamma^k R_{t+k+1} \big)}{\sum_{t=1}^T \mathbb{1}[S_t = s, A_t = a]}$$.


### Apprentissage par différence temporelle

Comme les méthodes de Monte-Carlo, l'apprentissage par différence temporelle (TD) est sans modèle et apprend à partir d'épisodes d'expérience. Cependant, l'apprentissage TD peut apprendre à partir de <span style="color : #e01f1f ;">**incomplet**</span> épisodes et nous n'avons donc pas besoin de suivre l'épisode jusqu'à la fin. L'apprentissage TD est si important que Sutton et Barto (2017), dans leur livre sur l'apprentissage par renforcement, le décrivent comme " une idée... centrale et nouvelle pour l'apprentissage par renforcement ".


#### Bootstrapping

Les méthodes d'apprentissage TD actualisent les objectifs en fonction des estimations existantes plutôt que de s'appuyer exclusivement sur les récompenses réelles et les rendements complets comme dans les méthodes MC. Cette approche est connue sous le nom de **bootstrapping**.


#### Estimation de la valeur

L'idée clé de l'apprentissage TD est de mettre à jour la fonction de valeur $$V(S_t)$$ vers un rendement estimé $$R_{t+1} + \gamma V(S_{t+1})$$ (connu sous le nom de "**cible TD**"). Dans quelle mesure nous voulons mettre à jour la fonction de valeur est contrôlée par l'hyperparamètre de taux d'apprentissage α :

$$
\begin{aligned}
V(S_t) &\flèche gauche (1- \alpha) V(S_t) + \alpha G_t \\N
V(S_t) &\flèche gauche V(S_t) + \alpha (G_t - V(S_t)) \\
V(S_t) &\flèche gauche V(S_t) + \alpha (R_{t+1}) + \gamma V(S_{t+1}) - V(S_t))
\end{aligned}
$$

De même, pour l'estimation de la valeur de l'action :

$$
Q(S_t, A_t) \flèche gauche Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))
$$

Ensuite, entrons dans la partie amusante : comment apprendre la politique optimale dans l'apprentissage de la DT (alias " contrôle de la DT "). Soyez prêt, vous allez voir de nombreux noms célèbres d'algorithmes classiques dans cette section.


#### SARSA : Contrôle de la DT sur la politique

"SARSA" fait référence à la procédure de mise à jour de la valeur Q en suivant une séquence de $$\dots, S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}, \dots$$. L'idée suit le même chemin que [GPI](#policy-iteration). En un épisode, elle fonctionne comme suit :

1. Initialiser $$t=0$$. 
2. Commencez avec $$S_0$$ et choisissez l'action $$A_0 = \arg\max_{a \in \mathcal{A}} Q(S_0, a)$$, où l'on applique couramment la méthode $$\epsilon$$-greedy.
3. Au temps $$t$$, après avoir appliqué l'action $$A_t$$, on observe la récompense $$R_{t+1}$$ et on passe à l'état suivant $$S_{t+1}$$.
4. On choisit ensuite l'action suivante de la même manière qu'à l'étape 2 : $$A_{t+1} = \arg\max_{a \in \mathcal{A}} Q(S_{t+1}, a)$$.
5. Mettre à jour la fonction de valeur Q : $$ Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha (R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)) $$.
6. Définir $$t = t+1$$ et répéter à partir de l'étape 3.

A chaque étape de SARSA, nous devons choisir la *prochaine* action en fonction de la politique *courante*.


#### Q-Learning : Contrôle de la TD hors politique

Le développement du Q-learning ([Watkins & Dayan, 1992](https://link.springer.com/content/pdf/10.1007/BF00992698.pdf)) est une grande découverte des premiers jours de l'apprentissage par renforcement. En un épisode, il fonctionne comme suit :

1. Initialiser $$t=0$$. 
2. Commence par $$S_0$$.
3. Au pas de temps $$t$$, on choisit l'action en fonction des valeurs de Q, $$A_t = \arg\max_{a \in \mathcal{A}} Q(S_t, a)$$ et on applique couramment la méthode $$epsilon$$-greedy.
4. Après avoir appliqué l'action $$A_t$$, on observe la récompense $$R_{t+1}$$ et on passe à l'état suivant $$S_{t+1}$$.
5. Actualisez la fonction de valeur Q : $$Q(S_t, A_t) \flèche gauche Q(S_t, A_t) + \alpha (R_{t+1} + \gamma \max_{a \in \mathcal{A}} Q(S_{t+1}, a) - Q(S_t, A_t))$$.
4. $$t = t+1$$ et répéter à partir de l'étape 3.

La principale différence avec SARSA est que le Q-learning ne suit pas la politique actuelle pour choisir la deuxième action $$A_{t+1}$$. Il estime $$Q^*$$ parmi les meilleures valeurs de Q, mais l'action (désignée par $$a^*$$) qui conduit à ce Q maximal n'a pas d'importance et, à l'étape suivante, l'apprentissage Q peut ne pas suivre $$a^*$$.


![SARSA et Q-learning]({{ '/assets/images/sarsa_vs_q_learning.png' | relative_url }})
{ : style="width : 50% ;" class="center"}
*Fig. 6. Les diagrammes de sauvegarde pour Q-learning et SARSA. (Source de l'image : Reproduit à partir de la figure 6.5 de Sutton & Barto (2017))*.


#### Réseau Q profond

Théoriquement, on peut mémoriser $$Q_*(.)$$ pour toutes les paires état-action en Q-learning, comme dans un gigantesque tableau. Cependant, cela devient rapidement infaisable sur le plan informatique lorsque l'espace d'état et d'action est grand. C'est pourquoi les gens utilisent des fonctions (c'est-à-dire un modèle d'apprentissage automatique) pour approximer les valeurs de Q. C'est ce qu'on appelle **l'approximation par fonction**. Par exemple, si nous utilisons une fonction avec le paramètre $$\theta$$ pour calculer les valeurs Q, nous pouvons étiqueter la fonction de valeur Q comme $$Q(s, a ; \theta)$$.

Malheureusement, l'apprentissage Q peut souffrir d'instabilité et de divergence lorsqu'il est associé à une approximation non linéaire de la fonction de valeur Q et au [bootstrapping](#bootstrapping) (voir [Problèmes #2](#deadly-triad-issue)).

Deep Q-Network ("DQN" ; Mnih et al. 2015) vise à améliorer considérablement et à stabiliser la procédure de formation de Q-learning grâce à deux mécanismes innovants :
- **Reprise de l'expérience** : Toutes les étapes de l'épisode $$e_t = (S_t, A_t, R_t, S_{t+1})$$ sont stockées dans une mémoire de relecture $$D_t = \{ e_1, \dots, e_t \}$$. $$D_t$$ contient des tuples d'expérience sur de nombreux épisodes. Pendant les mises à jour de Q-learning, les échantillons sont tirés au hasard de la mémoire de relecture et un échantillon peut donc être utilisé plusieurs fois. La relecture de l'expérience améliore l'efficacité des données, supprime les corrélations dans les séquences d'observation et lisse les changements dans la distribution des données.
- **Cible mise à jour périodiquement** : Q est optimisé vers des valeurs cibles qui ne sont mises à jour que périodiquement. Le réseau Q est cloné et maintenu figé comme cible d'optimisation tous les C pas (C est un hyperparamètre). Cette modification rend l'apprentissage plus stable car elle permet de surmonter les oscillations à court terme. 

La fonction de perte ressemble à ceci :

$$
\mathcal{L}(\theta) = \mathbb{E}_{(s, a, r, s') \sim U(D)} \Big[ \big( r + \gamma \max_{a'}) Q(s', a' ; \theta^{-}) - Q(s, a ; \theta) \big)^2 \Big]
$$

où $$U(D)$$ est une distribution uniforme sur la mémoire de relecture D ; $$\theta^{-}$$ est les paramètres du Q-réseau cible gelé.

En outre, il est également utile de réduire le terme d'erreur pour qu'il soit compris entre [-1, 1]. (J'ai toujours un sentiment mitigé à l'égard de l'écrêtage des paramètres, car de nombreuses études ont montré que cela fonctionne empiriquement, mais que cela rend les mathématiques beaucoup moins jolies :/).


![Algorithme DQN]({{ '/assets/images/DQN_algorithm.png' | relative_url }})
{ : style="width : 75% ;" class="center"}
*Fig. 7. Algorithme pour DQN avec relecture de l'expérience et cible d'optimisation occasionnellement gelée. La séquence préposée est la sortie de certains processus s'exécutant sur les images d'entrée des jeux Atari. Ne vous en préoccupez pas trop ; considérez-les simplement comme des vecteurs de caractéristiques d'entrée. (Source de l'image : Mnih et al. 2015)*. 


Il existe de nombreuses extensions du DQN pour améliorer la conception originale, comme le DQN avec architecture de duel (Wang et al. 2016) qui estime la fonction état-valeur V(s) et la fonction avantage A(s, a) avec des paramètres de réseau partagés. 


### Combinaison de l'apprentissage TD et MC

Dans la [section](#value-estimation) précédente sur l'estimation de la valeur dans l'apprentissage de la DT, nous ne retraçons qu'une étape de la chaîne d'action lors du calcul de la cible de la DT. On peut facilement l'étendre à plusieurs étapes pour estimer le rendement. 

Appelons le rendement estimé après n étapes $$G_t^{(n)}, n=1, \dots, \infty$$, alors :

{ : class="info"}
| $$n$$ | $$G_t$$ | Notes |
| ------------- | ------------- | ------------- |
| $$n=1$$ | $$G_t^{(1)} = R_{t+1} + \gamma V(S_{t+1})$$ | apprentissage TD |
| $$n=2$$ | $$G_t^{(2)} = R_{t+1} + \gamma R_{t+2} + \gamma^2 V(S_{t+2})$$ | | |
| ... | | |
| $$n=n$$ | $$ G_t^{(n)} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{n-1} R_{t+n} + \gamma^n V(S_{t+n}) $$ | | |
| ... | | |
| $$n=\infty$$ | $$G_t^{(\infty)} = R_{t+1} + \gamma R_{t+2} + \dots + \gamma^{T-t-1} R_T + \gamma^{T-t} V(S_T) $$ | Estimation MC |

L'apprentissage TD généralisé à n étapes a toujours la [même] forme (#estimation de la valeur) pour la mise à jour de la fonction de valeur :

$$
V(S_t) \leftarrow V(S_t) + \alpha (G_t^{(n)}) - V(S_t))
$$

![TD lambda]({{ '/assets/images/TD_lambda.png' | relative_url }})
{ : style="width : 70% ;" class="center"}


Nous sommes libres de choisir n'importe quel $$n$$ dans l'apprentissage de la TD comme nous le souhaitons. La question est maintenant de savoir quel est le meilleur $$n$$ ? Quel $$G_t^{(n)}$$ nous donne la meilleure approximation du rendement ? Une solution commune mais intelligente consiste à appliquer une somme pondérée de toutes les cibles TD possibles à n étapes plutôt que de choisir une seule meilleure n. Les poids diminuent d'un facteur λ avec n, $$\lambda^{n-1}$$ ; l'intuition est similaire à [pourquoi](#estimation de la valeur) nous voulons escompter les récompenses futures lors du calcul du rendement : plus nous regardons vers l'avenir, moins nous serons confiants. Pour que la somme de tous les poids (n → ∞) soit égale à 1, nous multiplions chaque poids par (1-λ), car :

$$
\begin{aligned}
\N- Let } S &= 1 + \lambda + \lambda^2 + \dots \\N
S &= 1 + \lambda(1 + \lambda + \lambda^2 + \dots) \\\N
S &= 1 + \lambda S \\N
S &= 1 / (1-\lambda)
\end{aligned}
$$

Cette somme pondérée de plusieurs rendements à n étapes est appelée λ-rendement $$G_t^{\lambda} = (1-\lambda) \sum_{n=1}^{\infty} \lambda^{n-1} G_t^{(n)}$$. L'apprentissage TD qui adopte le λ-retour pour la mise à jour de la valeur est étiqueté comme **TD(λ)**. La version originale que nous avons introduite [ci-dessus] (#value-estimation) est équivalente à **TD(0)**.


![Diagrammes de sauvegarde]({{ '/assets/images/TD_MC_DP_backups.png' | relative_url }})
{ : class="center"}
*Fig. 8. Comparaison des diagrammes de sauvegarde de Monte-Carlo, de l'apprentissage par différence temporelle et de la programmation dynamique pour les fonctions de valeur d'état. (Source de l'image : Cours RL de David Silver [cours 4] (http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MC-TD.pdf) : "Model-Free Prediction")*.


### Gradient de politique

Toutes les méthodes que nous avons présentées ci-dessus visent à apprendre la fonction de valeur état/action, puis à sélectionner des actions en conséquence. Les méthodes de gradient de politique apprennent au contraire directement la politique avec une fonction paramétrée par rapport à $$\theta$$, $$\pi(a \vert s ; \theta)$$. Définissons la fonction de récompense (opposée à la fonction de perte) comme *le rendement attendu* et entraînons l'algorithme dans le but de maximiser la fonction de récompense. Mon [next post]({{ site.baseurl }}{% post_url 2018-04-08-policy-gradient-algorithms %}) décrit pourquoi le théorème du gradient de politique fonctionne (preuve) et présente un certain nombre d'algorithmes de gradient de politique.

Dans un espace discret :

$$
\mathcal{J}(\theta) = V_{\pi_\theta}(S_1) = \mathbb{E}_{\pi_\theta}[V_1]
$$

où $$S_1$$ est l'état initial de départ. 

Ou dans l'espace continu :

$$
\mathcal{J}(\theta) = \sum_{s \in \mathcal{S} d_{\pi_\theta}(s) V_{\pi_\theta}(s) = \sum_{s \in \mathcal{S}} \Big( d_{\pi_\theta}(s) \sum_{a \in \mathcal{A}} \pi(a \vert s, \theta) Q_\pi(s, a) \Big)
$$

où $$d_{\pi_\theta}(s)$$ est la distribution stationnaire de la chaîne de Markov pour $$\pi_\theta$$. Si vous n'êtes pas familier avec la définition d'une "distribution stationnaire", veuillez consulter cette [référence] (https://jeremykun.com/2015/04/06/markov-chain-monte-carlo-without-all-the-bullshit/).

En utilisant *l'ascension progressive*, nous pouvons trouver le meilleur θ qui produit le rendement le plus élevé. Il est naturel de s'attendre à ce que les méthodes basées sur les politiques soient plus utiles dans un espace continu, car il existe un nombre infini d'actions et/ou d'états pour lesquels il faut estimer les valeurs dans un espace continu et, par conséquent, les approches basées sur les valeurs sont beaucoup plus coûteuses en termes de calcul.


#### Théorème du gradient de la politique

Le calcul du gradient *numérique* peut être fait en perturbant θ d'une petite quantité ε dans la k-ième dimension. Cela fonctionne même lorsque $$J(\theta)$$ n'est pas différentiable (sympa !), mais sans surprise très lent.

$$
\frac{\partial \mathcal{J}(\theta)}{\partial \theta_k} \approx \frac{\mathcal{J}(\theta + \epsilon u_k) - \mathcal{J}(\theta)}{\epsilon}
$$

Ou *analytiquement*,

$$
\mathcal{J}(\theta) = \mathbb{E}_{\pi_\theta} [r] = \sum_{s \in \mathcal{S} d_{\pi_\theta}(s) \sum_{a \in \mathcal{A}} \pi(a \vert s ; \theta) R(s, a)
$$

En fait, nous avons un bon support théorique pour (remplacer $$d(.)$$ par $$d_\pi(.)$$) :

$$
\mathcal{J}(\theta) = \sum_{s \in \mathcal{S} d_{\pi_\theta}(s) \sum_{a \in \mathcal{A}} \pi(a \vert s ; \theta) Q_\pi(s, a) \propto \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi(a \vert s ; \theta) Q_\pi(s, a)
$$

Consultez la section 13.1 de Sutton & Barto (2017) pour en connaître les raisons.

Ensuite,

$$
\begin{aligned}
\mathcal{J}(\theta) &= \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi(a \vert s ; \theta) Q_\pi(s, a) \\N
\nabla \mathcal{J}(\theta) &= \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \nabla \pi(a \vert s ; \theta) Q_\pi(s, a) \\\N
&= \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi(a \vert s ; \theta) \frac{\nabla \pi(a \vert s ; \theta)}{\pi(a \vert s ; \theta)} Q_\pi(s, a) \\\N
& = \sum_{s \in \mathcal{S}} d(s) \sum_{a \in \mathcal{A}} \pi(a \vert s ; \theta) \nabla \ln \pi(a \vert s ; \theta) Q_\pi(s, a) \\\\N
& = \mathbb{E}_{\pi_\theta} [\nabla \ln \pi(a \vert s ; \theta) Q_\pi(s, a)]
\end{aligned}
$$

Ce résultat est appelé "Théorème du gradient de politique" et constitue la base théorique de divers algorithmes de gradient de politique :

$$
\nabla \mathcal{J}(\theta) = \mathbb{E}_{\pi_\theta} [\nabla \ln \pi(a \vert s, \theta) Q_\pi(s, a)]
$$


#### REINFORCE

REINFORCE, également connu sous le nom de gradient de politique de Monte-Carlo, s'appuie sur $$Q_\pi(s, a)$$, un rendement estimé par les méthodes [MC](#monte-carlo-methods) à partir d'échantillons d'épisodes, pour mettre à jour le paramètre de politique $$\theta$$.

Une variante couramment utilisée de REINFORCE consiste à soustraire une valeur de base du retour $$G_t$$ pour réduire la variance de l'estimation du gradient tout en gardant le biais inchangé. Par exemple, une ligne de base commune est la valeur d'état, et si elle est appliquée, nous utiliserions $$A(s, a) = Q(s, a) - V(s)$$ dans la mise à jour de l'ascension du gradient.

1. Initialiser θ au hasard
2. Générer un épisode $$S_1, A_1, R_2, S_2, A_2, \dots, S_T$$$.
3. Pour t=1, 2, .... , T :
	1. Estimez le rendement G_t depuis le pas de temps t.
	2. $$\theta \leftarrow \theta + \alpha \gamma^t G_t \nabla \ln \pi(A_t \vert S_t, \theta)$$.


#### Acteur-Critique

Si la fonction de valeur est apprise en plus de la politique, nous obtenons l'algorithme Acteur-Critique.
- **Critique** : met à jour les paramètres de la fonction de valeur w et, selon l'algorithme, il peut s'agir d'une valeur d'action $$Q(a \vert s ; w)$$ ou d'une valeur d'état $$V(s ; w)$$.
- **Acteur** : met à jour les paramètres de la politique θ, dans la direction suggérée par le critique, $$\pi(a \vert s ; \theta)$$.

Voyons comment cela fonctionne dans un algorithme acteur-critique à valeur d'action. 

1. Initialiser s, θ, w au hasard ; échantillonner $$a \sim \pi(a \vert s ; \theta)$$.
2. Pour t = 1... T :
	1. Echantillon de récompense $$r_t \sim R(s, a)$$ et état suivant $$s' \sim P(s' \vert s, a)$$.
	2. Puis échantillonner l'action suivante $$a' \sim \pi(s', a' ; \theta)$$.
	3. Actualiser les paramètres de la politique : $$\theta \leftarrow \theta + \alpha_\theta Q(s, a ; w) \nabla_\theta \ln \pi(a \vert s ; \theta)$$.
	4. Calculez la correction pour la valeur de l'action au temps t : < br/>
	$$G_{t:t+1} = r_t + \gamma Q(s', a' ; w) - Q(s, a ; w)$$ < br/>
	et l'utiliser pour mettre à jour les paramètres de la fonction de valeur : < br/>
	$$w \Nflèche gauche w + \Nalpha_w G_{t:t+1} \nabla_w Q(s, a ; w) $$$.
	5. Mettez à jour $$a \leftarrow a'$$ et $$s \leftarrow s'$$.

$$\alpha_\theta$$ et $$\alpha_w$$ sont deux taux d'apprentissage pour les mises à jour des paramètres de la politique et de la fonction de valeur, respectivement.


#### A3C

**Asynchronous Advantage Actor-Critic** (Mnih et al., 2016), en abrégé A3C, est une méthode classique de gradient de politique avec l'accent particulier sur l'entraînement parallèle. 

Dans A3C, les critiques apprennent la fonction état-valeur, $$V(s ; w)$$, tandis que plusieurs acteurs sont formés en parallèle et sont synchronisés avec les paramètres globaux de temps en temps. Par conséquent, A3C est bon pour l'apprentissage parallèle par défaut, c'est-à-dire sur une machine avec un CPU multi-core.

La fonction de perte pour la valeur d'état consiste à minimiser l'erreur quadratique moyenne, $$\mathcal{J}_v (w) = (G_t - V(s ; w))^2$$ et nous utilisons la descente de gradient pour trouver le w optimal. Cette fonction de valeur d'état est utilisée comme ligne de base dans la mise à jour du gradient de la politique.

Voici le schéma de l'algorithme :
1. Nous avons des paramètres globaux, θ et w ; des paramètres similaires spécifiques aux fils, θ' et w'.
2. Initialiser le pas de temps t = 1
3. Tandis que T <= T_MAX :
	1. Remise à zéro du gradient : dθ = 0 et dw = 0.
	2. Synchroniser les paramètres spécifiques au thread avec les paramètres globaux : θ' = θ et w' = w.
	3. $$t_\text{start}$$ = t et on obtient $$s_t$$.
	4. Pendant que ($$s_t \neq \text{TERMINAL}$$) et ($$t - t_\text{start} <= t_\text{max}$$) :
		1. Choisir l'action $$a_t \sim \pi(a_t \vert s_t ; \theta')$$ et recevoir une nouvelle récompense $$r_t$$ et un nouvel état $$s_{t+1}$$.
		2. Mettre à jour t = t + 1 et T = T + 1.
	5. Initialiser la variable qui contient l'estimation du rendement $$R = \begin{cases} 
		0 & & \text{if } s_t \text{\c} est TERMINAL} \\
		V(s_t ; w') & \text{sinon}
		\end{cases}$$.
	6. Pour $$i = t-1, \dots, t_\text{start}$$ :
		1. $$R \leftarrow r_i + \gamma R$$ ; ici R est une mesure MC de $$G_i$$.
		2. Accumuler les gradients par rapport à θ' : $$d\theta \leftarrow d\theta + \nabla_{\theta'} \log \pi(a_i \vert s_i ; \theta')(R - V(s_i ; w'))$$$;< br/>
		Accumuler les gradients en fonction de w' : $$dw \leftarrow dw + \nabla_{w'} (R - V(s_i ; w'))^2$$.
	7. Mettre à jour de manière synchrone θ en utilisant dθ, et w en utilisant dw.

A3C permet le parallélisme dans la formation d'agents multiples. L'étape d'accumulation du gradient (6.2) peut être considérée comme une reformation de la mise à jour stochastique du gradient basée sur les minibatchs : les valeurs de w ou θ sont corrigées d'un petit peu dans la direction de chaque fil d'entraînement indépendamment.


### Stratégies d'évolution

Les stratégies d'évolution [Evolution Strategies](https://en.wikipedia.org/wiki/Evolution_strategy) (ES) sont un type d'approche d'optimisation agnostique. Elle apprend la solution optimale en imitant la théorie de Darwin sur l'évolution des espèces par sélection naturelle. Il y a deux conditions préalables à l'application de l'ES : (1) nos solutions peuvent interagir librement avec l'environnement et voir si elles peuvent résoudre le problème ; (2) nous sommes en mesure de calculer un score de **fitness** pour évaluer la qualité de chaque solution. Nous n'avons pas besoin de connaître la configuration de l'environnement pour résoudre le problème. 

Disons que nous commençons avec une population de solutions aléatoires. Toutes sont capables d'interagir avec l'environnement et seuls les candidats ayant un score de fitness élevé peuvent survivre (*seuls les plus aptes peuvent survivre dans une compétition pour des ressources limitées*). Une nouvelle génération est ensuite créée en recombinant les paramètres (*mutation génique*) des survivants à forte aptitude. Ce processus est répété jusqu'à ce que les nouvelles solutions soient suffisamment bonnes.

Très différente des approches populaires basées sur les MDP, comme celles que nous avons présentées ci-dessus, ES vise à apprendre le paramètre de politique $$\theta$$ sans approximation de valeur. Supposons que la distribution sur le paramètre $$\theta$$ soit une gaussienne multivariée [isotrope] (https://math.stackexchange.com/questions/1991961/gaussian-distribution-is-isotropic) avec une moyenne $$\mu$$ et une covariance fixe $$\sigma^2I$$. Le gradient de $$F(\theta)$$ est calculé :

$$
\begin{aligned}
& \nabla_\theta \mathbb{E}_{\theta \sim N(\mu, \sigma^2)} F(\theta) \\\N
=& \nabla_\theta \int_\theta F(\theta) \Pr(\theta) && \text{Pr(.) est la fonction de densité gaussienne.} \\
=& \int_\theta F(\theta) \Pr(\theta) \frac{\nabla_\theta \Pr(\theta)}{\Pr(\theta)} \\
=& \int_\theta F(\theta) \Pr(\theta) \nabla_\theta \log \Pr(\theta) \\\N
=& \mathbb{E}_{\theta \sim N(\mu, \sigma^2)} [F(\theta) \nabla_\theta \log \Pr(\theta)] && \text{Comme pour la mise à jour du gradient de la politique}. \\
=& \mathbb{E}_{\theta \sim N(\mu, \sigma^2)} \Big[ F(\theta) \nabla_\theta \log \Big( \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(\theta - \mu)^2}{2 \sigma^2 }} \Big) \Big] \\
=& \mathbb{E}_{\theta \sim N(\mu, \sigma^2)} \Big[ F(\theta) \nabla_\theta \Big( -\log \sqrt{2\pi\sigma^2} - \frac{(\theta - \mu)^2}{2 \sigma^2} \Big) \Big] \\
=& \mathbb{E}_{\theta \sim N(\mu, \sigma^2)} \Big[ F(\theta) \frac{\theta - \mu}{\sigma^2} \Big]
\end{aligned}
$$


Nous pouvons réécrire cette formule en termes de paramètre "moyen" $$\theta$$ (différent du $$\theta$$ ci-dessus ; ce $$\theta$$ est le gène de base pour les mutations ultérieures), $$\epsilon \sim N(0, I)$$ et donc $$\theta + \epsilon \sigma \sim N(\theta, \sigma^2)$$. $$\epsilon$$ contrôle la quantité de bruits gaussiens à ajouter pour créer la mutation :

$$
\nabla_\theta \mathbb{E}_{\epsilon \sim N(0, I)} F(\theta + \sigma \epsilon) = \frac{1}{\sigma} \mathbb{E}_{\epsilon \sim N(0, I)} [F(\theta + \sigma \epsilon) \epsilon]
$$


![EA]({{ '/assets/images/EA_RL_parallel.png' | relative_url }})
{ : class="center"}
*Fig. 9. Un algorithme RL parallèle simple basé sur des stratégies d'évolution. Les travailleurs parallèles partagent les graines aléatoires afin qu'ils puissent reconstruire les bruits gaussiens avec une bande passante de communication minuscule. (Source de l'image : Salimans et al. 2017.)*


ES, en tant qu'algorithme d'optimisation à boîte noire, est une autre approche des problèmes de RL (<span style="color : #999999 ;">*Dans mon texte original, j'ai utilisé l'expression "une bonne alternative" ; [Seita](https://danieltakeshi.github.io/) m'a signalé cette [discussion](https://www.reddit.com/r/MachineLearning/comments/6gke6a/d_requesting_openai_to_justify_the_grandiose/dir9wde/) et j'ai donc mis à jour ma formulation.*</span>). Il possède quelques bonnes caractéristiques (Salimans et al., 2017) qui le rendent rapide et facile à entraîner :
- ES n'a pas besoin d'approximation de la fonction de valeur ;
- L'ES n'effectue pas de rétropropagation du gradient ;
- L'ES est invariant par rapport aux récompenses différées ou à long terme ;
- ES est hautement parallélisable avec très peu de communication de données.


## Problèmes connus

### Dilemme de l'exploration-exploitation

Le problème du dilemme exploration vs exploitation a été abordé dans mon précédent [post]({{ site.baseurl }}{% post_url 2018-01-23-the-multi-armed-bandit-problem-and-its-solutions %}#exploitation-vs-exploration). Lorsque le problème RL fait face à un environnement inconnu, cette question est particulièrement une clé pour trouver une bonne solution : sans suffisamment d'exploration, nous ne pouvons pas apprendre l'environnement suffisamment bien ; sans suffisamment d'exploitation, nous ne pouvons pas compléter notre tâche d'optimisation de la récompense.

Les différents algorithmes de RL équilibrent l'exploration et l'exploitation de différentes manières. Dans les méthodes [MC](#monte-carlo-methods), [Q-learning](#q-learning-off-policy-td-control) ou de nombreux algorithmes on-policy, l'exploration est généralement mise en œuvre par [ε-greedy]({{ site.baseurl }}{% post_url 2018-01-23-the-multi-armed-bandit-problem-and-its-solutions %}#%CE%B5-greedy-algorithm) ; Dans [ES](#evolution-strategies), l'exploration est capturée par la perturbation du paramètre de politique. Veuillez en tenir compte lors du développement d'un nouvel algorithme RL.

### Numéro de la triade mortelle

Nous recherchons l'efficacité et la flexibilité des méthodes de TD qui font appel au bootstrapping. Cependant, lorsque le hors-politique, l'approximation de fonction non linéaire et le bootstrapping sont combinés dans un algorithme RL, la formation pourrait être instable et difficile à converger. Ce problème est connu sous le nom de **triade fatale** (Sutton & Barto, 2017). De nombreuses architectures utilisant des modèles d'apprentissage profond ont été proposées pour résoudre le problème, notamment le DQN pour stabiliser la formation avec le rejeu de l'expérience et le réseau cible occasionnellement gelé.


## Étude de cas : AlphaGo Zero

Le jeu de [Go](https://en.wikipedia.org/wiki/Go_(jeu)) a été un problème extrêmement difficile dans le domaine de l'intelligence artificielle pendant des décennies jusqu'à ces dernières années. AlphaGo et AlphaGo Zero sont deux programmes développés par une équipe de DeepMind. Ils font tous deux appel à des réseaux neuronaux convolutifs profonds ([CNN]({{ site.baseurl }}{% post_url 2017-12-15-object-recognition-for-dummies-part-2 %}#cnn-for-image-classification)) et à la recherche arborescente de Monte Carlo (MCTS) et ont tous deux été approuvés pour atteindre le niveau des joueurs de Go humains professionnels. Contrairement à AlphaGo, qui s'appuyait sur un apprentissage supervisé à partir de mouvements humains experts, AlphaGo Zero n'a utilisé que l'apprentissage par renforcement et l'auto-exécution, sans connaissance humaine au-delà des règles de base.

![Plateau de jeu de Go]({{ '/assets/images/go_config.png' | relative_url }})
{ : class="center"}
*Fig. 10. Le plateau de Go. Deux joueurs jouent alternativement des pierres noires et blanches sur les intersections libres d'un plateau de 19 x 19 lignes. Un groupe de pierres doit avoir au moins un point ouvert (une intersection, appelée "liberté") pour rester sur le plateau et doit avoir au moins deux libertés fermées ou plus (appelées "yeux") pour rester "vivant". Aucune pierre ne doit répéter une position précédente.*

Avec toutes les connaissances de RL ci-dessus, jetons un coup d'œil sur le fonctionnement d'AlphaGo Zero. Le composant principal est un [CNN]({{ site.baseurl }}{% post_url 2017-12-15-object-recognition-for-dummies-part-2 %}#cnn-for-image-classification) profond sur la configuration du plateau de jeu (précisément, un [ResNet]({{ site.baseurl }}{% post_url 2017-12-15-object-recognition-for-dummies-part-2 %}#resnet-he-et-al-2015) avec normalisation par lots et ReLU). Ce réseau produit deux valeurs :

$$
(p, v) = f_\theta(s)
$$

- $$s$$ : la configuration du plateau de jeu, 19 x 19 x 17 plans de caractéristiques empilés ; 17 caractéristiques pour chaque position, 8 configurations passées (y compris la configuration actuelle) pour le joueur actuel + 8 configurations passées pour l'adversaire + 1 caractéristique indiquant la couleur (1=noir, 0=blanc). Nous devons coder la couleur spécifiquement parce que le réseau joue avec lui-même et que les couleurs du joueur actuel et des adversaires changent entre les étapes.
- $$p$$ : la probabilité de choisir un coup parmi 19^2 + 1 candidats (19^2 positions sur le plateau, en plus de passer).
- $$v$$ : la probabilité de gagner compte tenu du réglage actuel.

Pendant l'auto-jeu, le SCTM améliore encore la distribution de probabilité de l'action $$\pi \sim p(.)$$ et ensuite l'action $$a_t$$ est échantillonnée à partir de cette politique améliorée. La récompense $$z_t$$ est une valeur binaire indiquant si le joueur actuel gagne *éventuellement* la partie. Chaque coup génère un tuple d'épisodes $$(s_t, \pi_t, z_t)$$ et il est sauvegardé dans la mémoire de relecture. Les détails sur les SCTM ne sont pas abordés dans ce billet pour des raisons d'espace ; veuillez lire le [document] original (https://www.dropbox.com/s/yva172qos2u15hf/2017-silver.pdf?dl=0) si vous êtes intéressé.


![Entraînement d'AlphaGo Zero]({{ '/assets/images/alphago-zero-selfplay.png' | relative_url }})
{ : class="center"}
*Fig. 11. AlphaGo Zero est entraîné par l'auto-exécution tandis que le MCTS améliore encore la politique de sortie à chaque étape. (Source de l'image : Figure 1a dans Silver et al., 2017).*

Le réseau est entraîné avec les échantillons dans la mémoire de relecture pour minimiser la perte :

$$
\mathcal{L} = (z - v)^2 - \pi^\top \log p + c \\N-theta \|^^2
$$

où $$c$$ est un hyperparamètre contrôlant l'intensité de la pénalité L2 pour éviter un surajustement.

AlphaGo Zero a simplifié AlphaGo en supprimant l'apprentissage supervisé et en fusionnant les réseaux de politiques et de valeurs séparés en un seul. Il s'avère qu'AlphaGo Zero a obtenu des performances largement améliorées avec un temps d'entraînement beaucoup plus court ! Je recommande vivement la lecture de ces [deux](https://pdfs.semanticscholar.org/1740/eb993cc8ca81f1e46ddaadce1f917e8000b5.pdf) (https://www.dropbox.com/s/yva172qos2u15hf/2017-silver.pdf?dl=0) côte à côte et de comparer les différences, c'est très amusant.


# <span style="color: #FF0000"> **Références** </span>

- [Deep reinforcement learning : An overview](https://arxiv.org/pdf/1701.07274.pdf) de Yuxi Li (2017)

- [Apprentissage par renforcement : une introduction ; 2e édition](http://incompleteideas.net/book/bookdraft2017nov5.pdf) de Richard S. Sutton et Andrew G. Barto (2017)

- [Méthodes asynchrones pour l'apprentissage par renforcement profond](http://proceedings.mlr.press/v48/mniha16.pdf) de Volodymyr Mnih et al. (2016)

- [Les stratégies d'évolution comme alternative évolutive à l'apprentissage par renforcement](https://arxiv.org/pdf/1703.03864.pdf) de Tim Salimans et al. (2017)

- [Maîtriser le jeu de go sans connaissance humaine](https://www.dropbox.com/s/yva172qos2u15hf/2017-silver.pdf?dl=0) de David Silver et al. (2017)

- [Maîtriser le jeu de Go avec des réseaux neuronaux profonds et la recherche d'arbres](https://pdfs.semanticscholar.org/1740/eb993cc8ca81f1e46ddaadce1f917e8000b5.pdf) de David Silver et al. (2016)

- [Human-level control through deep reinforcement learning](https://www.cs.swarthmore.edu/~meeden/cs63/s15/nature15b.pdf) de Volodymyr Mnih et al. (2015)

- [Dueling network architectures for deep reinforcement learning](https://arxiv.org/pdf/1511.06581.pdf) de Ziyu Wang et al. (2016)

- [Conférences sur l'apprentissage par renforcement](https://www.youtube.com/playlist?list=PL7-jPKtc4r78-wCZcQn5IqyuWhBZ8fOxT) de David Silver sur YouTube.

- OpenAI Blog : [Les stratégies d'évolution comme alternative évolutive à l'apprentissage par renforcement](https://blog.openai.com/evolution-strategies/)

- [gradients de politique d'exploration des paramètres](https://mediatum.ub.tum.de/doc/1287490/file.pdf) de Frank Sehnke et al.  (2010)

- [Algorithmes pour l'apprentissage par renforcement](https://sites.ualberta.ca/~szepesva/papers/RLAlgsInMDPs.pdf) de Csaba Szepesvári (2010)

