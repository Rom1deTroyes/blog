0:00,0:06
So welcome to class Wednesday 9 30 in the morning New York city and

0:06,0:11
today we have a guest lecturer for from Awni Hannun and Awni 

0:11,0:17
is a research scientist at the Facebook ai research lab focusing on low resource machine

0:17,0:23
learning speech recognition and privacy he earned a PhD in computer science from Stanford university

0:23,0:29
prior to Facebook he worked as a research scientist at in Baidu Silicon Valley ai lab where he

0:29,0:34
co-led the deep speech project and so thank you again Awni for

0:34,0:43
for joining us today i'm looking forward to listen to your awesome lectures and so as i remind all

0:43,0:48
the students if they have any kind of question during the class they can as usual type in the chat i

0:48,0:53
will be reading live all questions to you and such that we are going to be all on the same page

0:53,0:59
is it fine for you right yes okay so i will just eclipse from here and i will just you

0:59,1:05
know be joining with the audio only okay okay perfect

1:05,1:11
great i'll get started thanks for having me everyone so i'm going to talk about speech

1:11,1:17
recognition and graph transformer networks okay so here's a quick outline of what

1:17,1:23
i'd like to cover today first we'll start with a pretty high level introduction to speech recognition and

1:23,1:29
modern speech recognition specifically why it's become so good but what are

1:29,1:35
some of the problems still and then i'd like to do a pretty deep dive into the connectionist temporal

1:35,1:42
classification loss one of the primary criteria we use to train state of the art speech recognition system so i'd

1:42,1:50
really like people to leave with kind of a good understanding of how that works and then a similar

1:50,1:56
discussion of beam search decoding so how do we produce a good transcription given that we've

1:56,2:01
trained our speech system and then i'd like to leave you all with some kind of

2:01,2:07
exciting revivification of graph transformer

2:07,2:13
networks that Iâ€™ve been working on myself that Yann and others at fair Leon with two namely

2:13,2:19
worked on in the in the 80s and 90s and i'll talk about about those and what what we're doing

2:19,2:26
with them and how they work great so you know if you have questions please don't hesitate to ask

2:26,2:31
especially as we get into some of the more technical content i really do want people to feel free

2:31,2:37
to ask questions okay so modern speech recognition

2:37,2:44
just so everyone's on the same page our goal is we start with a snippet of audio some speech we feed

2:44,2:50
it through our speech recognizer and we desire the transcription so in this case the transcription would

2:50,2:55
be the quick brown fox jumps over the lazy dog

2:55,3:02
just a bit of fun trivia the reason you see that phrase so often is because it's actually a

3:02,3:10
pangram which means that it uses every letter in the English alphabet it's not the shortest

3:10,3:16
one but it is a pen gram so why has automatic speech recognition

3:16,3:22
gotten so good it has really improved a lot since well

3:22,3:29
since a while but particularly since 2012 it's really continued to get rapidly better and on some

3:29,3:35
academic benchmarks if we compare the performance to say human level performance

3:35,3:40
if i were to ask humans to transcribe the same speech we'd see that the machine

3:40,3:46
performance is as good or better so it's really good on some of these benchmarks

3:46,3:54
but it's not solved there are still places where speech recognition struggles in particular conversational

3:54,3:59
speech if you and your friend have a very lively conversation

3:59,4:05
that's going to be hard for a speech recognizer to transcribe well lots of background noise and then

4:05,4:10
importantly when there are underrepresented groups

4:10,4:16
where by underrepresented I mean their accents or certain features about them are not well

4:16,4:21
represented in the training data the speech recognition doesn't work well

4:21,4:26
and this is real I mean and this is recent you'll find articles about bias and

4:26,4:32
speech recognition from just a few weeks ago

4:32,4:38
and their claim is that there is still considerable bias in speech recognition

4:38,4:45
a state of the art speech struggles with things like gender age speech impairments

4:45,4:50
and accents to name a few so you know you know Facebook and other

4:50,4:56
places we really strive to have speech recognition which works in many languages hundreds of languages

4:56,5:02
in many contexts across the board of types of

5:02,5:10
people and types of speech so you know we're a long ways away from having solutions for that

5:10,5:16
so I said i asked the question why has why has asr gotten so much better you know it's not solved but it has

5:16,5:23
undeniably gotten much better so let's talk about why

5:23,5:29
so before 2012 there were some problems with speech recognition systems i'll kind of call these more like traditional

5:29,5:34
speech recognition systems one of the problems was that

5:34,5:41
they consisted of lots of many hand-engineered components i call this alphabet soup because you'll find

5:41,5:48
just acronyms littered in speech systems from pre-2012 and because of

5:48,5:54
the these this you know kind of superfluous number of hand-engineered components it

5:54,6:00
really meant that when i came to add more data for example

6:00,6:05
it didn't really help because the models were overly hand designed and not able to

6:05,6:12
learn well together from data so data sets remain small and larger data sets were not useful

6:12,6:19
combining these modules only at inference time instead of learning them together

6:19,6:24
allowed for errors to cascade they didn't learn to work well together

6:24,6:30
and then importantly and often underestimated is when you have really complex system like this

6:30,6:36
it's hard for researchers to know how to improve it so you know when i started my PhD I

6:36,6:42
didn't really know what to do to make a speech recognition system better and we worked on random parts of it and

6:42,6:48
it was fun to learn but we were kind of shooting in the dark especially at first the learning curve

6:48,6:53
was quite steep so you know that was that was difficult

6:53,7:01
so why has asr gotten better you've probably seen figures like this for many different applications the idea is the same

7:01,7:07
we replace a lot of the traditional components with deep learning thanks john and

7:07,7:14
and more data and the two together work in a virtuous cycle where you know

7:14,7:20
we add more data the deep models can get better makes more data more useful

7:20,7:26
so on and so forth so this is not meant to be understood

7:26,7:32
this is kind of a picture of what speech recognition looked like

7:32,7:38
pre-2012 consisted of these many different components you start with your speech on the on the left and you move it through all

7:38,7:43
these components featurization speaker adaptation so on so forth

7:43,7:50
all the way through this decoder which takes in a bunch of different models so it's complex and slowly we figured out well we can

7:50,7:56
get rid of some of these pieces and replace them with deep learning can replace speaker adaptation and some

7:56,8:03
of the smaller models with just a big acoustic model we can keep going we can get rid of

8:03,8:09
some of the components that specify how the

8:09,8:15
transcription is processed and simplify the pipeline further we can

8:15,8:20
keep going we can simplify even more so now we have a we're starting to look at what

8:20,8:27
looks like a production speech system today it's actually quite simple it's got some featurization from the speech

8:27,8:34
it's got an acoustic model which can be a fairly complex but single deep learning

8:34,8:41
deep neural network and then and then it produces word pieces or letters which go into a

8:41,8:46
decoder which you know tries to find the best transcription so it's already looking quite a bit simpler and then

8:46,8:52
and then this is in production and in research we've gone even further we haven't quite landed these

8:52,8:58
improvements i would call them into production yet but we're on the way so things like removing the decoder

8:58,9:05
entirely why do we need this complex decoder can we just produce a transcription directly

9:05,9:11
why do we even need features can we just learn from the raw audio directly so these are things that that we're

9:11,9:18
doing in research that are showing to help but they're still kind of either too expensive or don't help enough yet

9:18,9:26
or there just hasn't been enough time to get them into production so that that's where the that's where things are headed let's

9:26,9:34
talk about connectionist temporal classification so as i said earlier ctc

9:34,9:39
is one of the more commonly used these days loss functions for

9:39,9:46
training state-of-the-art speech recognition systems so i'd like everyone to kind of understand how that works and kind

9:46,9:55
of why it works the way that it does okay so as we said earlier we're given

9:55,10:00
some input speech and in this case it will be this utterance x

10:00,10:07
which consists of frames of audio so x is has t frames x1 to xt

10:07,10:13
each frame is usually in speech recognition usually like 20 milliseconds of speech

10:13,10:18
so we'll take you know it'll take a given snippet of speech and we'll chop it up into 20 millisecond

10:18,10:25
slices they overlap a little bit and that will be that will be the features that go into

10:25,10:32
the into the model and then we desire to produce a transcription which i'm calling y here

10:32,10:40
and we'll think of our transcription as consisting of the letters of a sentence so y1 is the first letter

10:40,10:47
y capital u is the last letter and so what do i need to train a model well i need the

10:47,10:56
ability to compute a loss i need to know how good a given transcription is

10:56,11:02
for a given piece of how good a transcription is given some input audio i need to compute

11:02,11:07
a score for that and then if i'm able to compute that score

11:07,11:13
i can then differentiate that score with respect to

11:13,11:19
some parameters and then bring all of our standard kind of optimization methods

11:19,11:26
to bear to improve the score by tuning the

11:26,11:32
parameters so to recap we need to compute this conditional

11:32,11:38
probability the probability of a transcription given an audio snippet

11:38,11:46
and ideally it should be differentiable with respect to the model parameters so that those would be like the parameters of a deep neural network

11:46,11:53
and then we can train the deep neural network to maximize this probability for our training set so we're given a

11:53,11:58
training set of y and x pairs and we want to maximize the probability

11:58,12:05
of all of those y of x pairs by kind of changing the parameters so that those scores are high

12:05,12:12
so how do we compute this score that's what that's what ctc is going to do for us we're going to build up towards ctc in a

12:12,12:19
sequence of steps and then and that will show us how to get this score

12:19,12:25
so the first question would be what is the input representation what are x1 2xt you said there are 20

12:25,12:30
milliseconds chunks but how do we encode this

12:30,12:36
good question so usually in say state-of-the-art production

12:36,12:42
speech systems the 20 milliseconds would be encoded as

12:42,12:49
what are called mel scale filter banks so you can think of these

12:49,12:56
as so the way that that works is i'll take kind of a 20 millisecond segment of

12:56,13:03
audio i'll compute the Fourier transform of just that 20 milliseconds

13:03,13:08
but instead of kind of binning frequencies evenly

13:08,13:15
i will bend them such that they correspond to how the human actually perceives speech

13:15,13:22
and so it turns out humans we perceive larger differences between lower frequencies

13:22,13:27
than higher frequencies so the bins are going to kind of

13:27,13:33
change in size in order to reflect that fact so that that's what the mel scale is so

13:33,13:41
it's basically like a short you know Fourier transform on 20 milliseconds and then we put those then

13:41,13:47
we've been the Fourier coefficients according to how humans perceive speech

13:47,13:55
and the choice of 20 milliseconds comes from 20 milliseconds that choice comes from

13:55,14:00
well it's a great question also it it it's really been used for a very long time now 20

14:00,14:07
milliseconds but it has been kind of cross-validated over considerably it's mostly an empirical

14:07,14:14
finding that it works well and it turns out it's a good trade-off between kind of time resolution and

14:14,14:19
frequency resolution so you know as you increase that window size you can get better frequency

14:19,14:26
resolution but you lose temporal resolution you lose the ability to distinguish between changes

14:26,14:31
on a on a fine-grained time scale and so you kind of need to trade that

14:31,14:38
off to have good performance and 20 milliseconds seems to work pretty well

14:38,14:45
okay makes sense great so where were we

14:45,14:51
okay so we're going to compute this score let's let's look at a quick example

14:51,14:57
let's just start with a really simple example so say you're given some audio you have

14:57,15:03
three frames here so this would be like you know 60 milliseconds of audio you never have such short audio in practice

15:03,15:09
but just bear with me so you have three frames x1 x2 x3

15:09,15:16
and then you have your output transcription which is the word cat and in letter c a t so

15:16,15:21
you'll notice there's three input frames and three output letters so we can do something really simple here

15:21,15:27
if i want to compute the score i can just kind of say well the first letter maps to the first input

15:27,15:34
frame the second letter maps to the second input frame and the third letter maps to the third input frame i'll just compute the scores of those

15:34,15:40
individually in log space i'll sum them up and i'll get my log probability

15:40,15:47
that's straightforward and it works well when the number of input frames

15:47,15:55
matches the number of output frames so you can see here you know we have

15:55,16:01
one second we have we have our we kind of have

16:01,16:13
our correspondence between inputs and outputs x1 x2 x3 and then the letter c a and t

16:13,16:22
okay so what happens if we have a fourth input frame what do we

16:22,16:28
put there i mean there's this problem now where i have to

16:28,16:34
decide how to score this fourth input frame right like i already

16:34,16:41
put c to the first input frame a to the second t to the third what do i what do i correspond to the fourth

16:41,16:48
and so maybe what i could do is something like this right where i can

16:48,16:56
decide to allow multiple inputs to map to one or more outputs

16:56,17:03
so in this example i have four input frames the last two inputs both map to

17:03,17:09
the t in the output so maybe that's what i want to do i'll call that an alignment between the input and

17:09,17:15
output so i can allow that t to repeat one or more times in other

17:15,17:20
words i can have each input mapped to one or more outputs [Music]

17:20,17:27
or rather each input map mapped to an output more than one time so there's multiple

17:27,17:32
ways i could do this for this example actually there's three different ways i

17:32,17:38
could have the a repeat twice in the middle i could have the c repeat twice at the beginning

17:38,17:45
or i could have the t repeat as in the first case so there's three possible alignments for this case

17:45,17:51
in other cases there will be many more but in this case there's only three but there's still the question of well

17:51,17:58
which alignment should i use to compute the score i mean if i were just gonna if i if i only had one alignment it

17:58,18:06
would be easy i would just use that one but since i have three possibilities it's not obvious which one i should pick

18:06,18:11
and ahead of time we don't know which one is the best because we don't know how the

18:11,18:18
transcription aligns to the input it's just given to us as cat and input is given to us as four frames

18:18,18:23
and the model maybe it prefers one over the other initially but that will just be arbitrary

18:23,18:29
so we don't really want to just let the model choose from the beginning

18:29,18:37
so in fact what we're going to do is use all of the alignments and

18:37,18:42
rather than fix one we're going to try to increase the score

18:42,18:48
of all of them and then kind of let them just hope that the model sorts things out

18:48,18:54
internally it can kind of decide to optimize these different

18:54,19:01
alignments and kind of weight them accordingly and learn which one is the best but

19:01,19:07
we're not going to tell it ahead of time which one to use we're just going to let it use all of them so here what i'm showing is that if we

19:07,19:16
want to compute the score of y given x we can take the sum of the of the probabilities of

19:16,19:21
all possible alignments for y 2x

19:21,19:26
and if you recall when i computed the score of an individual alignment a

19:26,19:33
couple slides ago that was in that was in log space so i got the log of the probability of the alignment

19:33,19:40
so it turns out when we do this this is kind of important for later we we're given scores of alignments that

19:40,19:46
are in log space and we need to actually compute the log of the sum of the

19:46,19:52
probabilities so i think you guys are familiar with this operation you call it actual SoftMax

19:52,19:58
which i like so i'm going to call it actual SoftMax also and we're going to use this actual

19:58,20:04
SoftMax to compute the actual SoftMax

20:04,20:09
of the scores the log scores of the alignments

20:09,20:16
so concretely if i'm given kind of two probabilities that are in log space Iâ€™d like to compute the log of the sum of those

20:16,20:22
probabilities and then actual SoftMax lets me do that

20:22,20:28
and if you also recall i think i think you learned this in a previous lecture there is a there's a stable way

20:28,20:34
to do this and it's very important that you use that stable way

20:34,20:39
because otherwise things will not work is one of the most important tricks for you know it's kind

20:39,20:45
of sequential data and especially long sequential data and machine learning

20:45,20:52
it's the log one plus kind of trick of removing the max so i think

20:52,20:58
you've all seen that before but that's important here so it's important that we do everything in log space for numerical stability and that

20:58,21:08
also lets us use this trick okay so we're gonna we're gonna use the actual soft max later

21:08,21:14
so we said okay so we're gonna use all possible alignments right so in this case when we

21:14,21:21
had cat with four frames we had three alignments um we're going to compute the actual soft max of all three of them

21:21,21:28
and then that's going to give us our score the log of the probability of our transcription given x so we're

21:28,21:36
good this works for us as an aside i'd like to show you

21:36,21:44
how we can encode the set of possible alignments of why the transcription cat

21:44,21:49
to an arbitrary length input as a graph so this is this is a graph

21:49,21:56
which represents the set of possible alignments of the word cat in in letters to an arbitrary length input and i want

21:56,22:04
to explain this graph because it's going to come up again when we start talking about graph transformer networks

22:04,22:10
so this graph it's you'll sometimes hear it called a weighted finite state acceptor

22:10,22:16
wfsa what it does is well it has a start state which is

22:16,22:22
the bold state at the beginning zero it has an accepting state which is the

22:22,22:28
cons the concentric circle mark three at the end and

22:28,22:35
what it does is on each edge there's a label and a slash to the to the right of the

22:35,22:41
label a weight the weight in this case are all zeros we don't care about the weights in this case in some cases we will care about the

22:41,22:47
weights but here we just care about the labels so

22:47,22:53
this graph is encoding the fact that i can it encodes a set of alignments basically

22:53,23:01
any path through this graph is a possible alignment so you know i can traverse the c one or

23:01,23:07
more times and then i get to the state from zero to one i can traverse the a

23:07,23:14
arc at the state 1 as many times as i like but then i have to actually kind of traverse a at least once to get

23:14,23:21
to the next state 2. and i can traverse the t on the self loop any number of times but i have to

23:21,23:28
actually traverse the t once to get to the state three which is my accepting state so this

23:28,23:36
graph says that you know any alignment must output at least c a t once each of them once

23:36,23:44
but for each of them it can also output them more than one time in succession and so this doesn't fix a length this

23:44,23:49
this encodes all possible lengths but it but it encodes the fact that you

23:49,23:54
know you have to output the letter in the transcription one or more times

23:54,24:01
so these graphs are going to come up again when we talk about graph transforming marks

24:01,24:06
okay so back to ctc so we were we were saying we're going to

24:06,24:12
use all possible alignments and that's great we're going to do that but there is a problem

24:12,24:18
and the problem is the x input audio can have lots of frames you can have lots of time

24:18,24:25
steps in practice this can be as high as a thousand the wide transcription can have lots of

24:25,24:31
letters in practice it can be 100 or more if you work out the numbers

24:31,24:37
this is an astronomically large number of alignments so we can't just do what i did earlier

24:37,24:42
which is kind of delineate all of them compute the score of each individual one

24:42,24:48
sum up all of those scores using our actual SoftMax that's not going to work in practice

24:48,24:55
as a fun little exercise i encourage you to kind of do the combinatorial math to

24:55,25:01
actually compute that number to show how many alignments you can actually produce

25:01,25:09
given an input of length t and an output of one two but it's a lot so what are we gonna do

25:09,25:16
well luckily there's an algorithm which lets us compute the sum over all

25:16,25:22
possible alignments efficiently it's called the forward algorithm in speech recognition

25:22,25:28
it's in it's basically a straightforward dynamic programming algorithm

25:28,25:35
and i believe you are experienced with the Viterbi algorithm

25:35,25:42
in a previous assignment or lecture and the forward algorithm is actually very similar the main difference

25:42,25:50
is kind of the operation that we use instead of kind of looking for the largest scoring or

25:50,25:56
shortest path we look for the sum of all possible paths so we'll

25:56,26:03
use a sum instead of a max or a min whichever you used in your in your assignment

26:03,26:08
so how does the forward algorithm work we'll talk about that for the simple

26:08,26:16
case of the alignments that i showed you earlier

26:16,26:24
okay so we start by specifying this forward variable and call it alpha subscript t superscript u

26:24,26:31
where the subscript index is kind of where we are in the input and the superscript indexes where we are

26:31,26:39
in the output and what this variable represents is the score for all alignments of length t

26:39,26:46
which end in the output y u so let's make that more concrete

26:46,26:53
so here's an example suppose i have the input the four

26:53,27:00
the four frame input sorry that's a typo there it should be x1 x2 x3 x4 and the output ycat and i want to

27:00,27:05
compute the forward variable alpha 2c so that

27:05,27:10
in words alpha 2c is the score of all possible alignments of

27:10,27:17
length 2 up to the first two frames that end in c in the first output of my

27:17,27:24
transcription and so there's only there's only one possible alignment which satisfies that

27:24,27:30
that's the alignment c c the first frame gets the c and the second frame gets the c

27:30,27:36
and so this this one is simple the compute it's just this the sum of the scores of the first two

27:36,27:42
of the first two frames of a p of c given the first frame and p

27:42,27:48
of c given the second similarly i can compute the forward

27:48,27:56
variable alpha sub 2 superscript a which is the in words the

27:56,28:02
the score of all possible alignments of length 2 which end in the second output

28:02,28:09
a and again there's only one possibility here there's the first input aligns to c

28:09,28:15
and the second input aligns to a so i can i can do the same thing we

28:15,28:21
compute the score of alpha 2a

28:21,28:28
okay now things get a little more complicated when i want to compute alpha 3a so i

28:28,28:33
want to compute the sum of all possible alignments of length

28:33,28:41
3 which end in the second output a and if you kind of think about it

28:41,28:48
there are two possible alignments there's c c a and c a a of length three

28:48,28:55
which end in a so i can delineate those two alignments i can compute their individual scores in

28:55,29:03
log space as i show below log p of a1 and log p of a2 that's the log score

29:03,29:10
of each of those two alignments and then i combine them using my actual SoftMax and that gives me

29:10,29:17
alpha 3 a so this would be the kind of the naive approach to compute alpha 3a

29:17,29:24
but if we kind of stare at these equations for long enough and

29:24,29:31
refer back to the previous two slides you'll notice that

29:31,29:37
the first part of the you know each equation is something

29:37,29:43
we've already computed so the equation for

29:43,29:50
the alignment that ends in a the first one consists of alpha 2c

29:50,29:57
plus the score and the second one consists of output to a plus the score so we can reuse those

29:57,30:04
alphas that we already computed and that makes things simpler

30:04,30:10
and more efficient this is the kind of the recursion that we're going to build up to so we can reuse those we just plug them

30:10,30:18
in and instead of instead of recomputing the prefix

30:18,30:29
we can compute the new score just by adding the score at the third frame of a given the third frame

30:29,30:36
so there's another observation which we can use which is that if we plug these

30:36,30:42
two scores into our actual soft max things factorize quite nicely

30:42,30:48
and it turns out that the actual soft max of alpha two c plus

30:48,30:56
alpha two a plus the log of the score of a at the third frame is actually this the

30:56,31:02
alpha 3a that we wanted to compute so you see here i can compute alpha 3a

31:02,31:08
just by adding the score of a at the third frame to

31:08,31:16
the previous two alphas at the alphas at the previous two time steps

31:16,31:23
so this leads us to a general recursion for computing the forward variables

31:23,31:31
and this general recursion this is kind of in the simple case for the alignments we've specified

31:31,31:37
looks like this so let's unpack this for a second

31:37,31:46
if i'm trying to compute the forward variable at time step t corresponding to output u

31:46,31:51
i will take the two forward variables at the previous time step

31:51,31:57
one which corresponds to output u and one which corresponds to output u minus one

31:57,32:04
and i will consider extending them by the output u so i'll take the actual

32:04,32:10
SoftMax of those two previous forward variables and i'll simply add to them the log of the

32:10,32:21
the output in my in my transcription given the teeth frame

32:21,32:27
so this is the forward algorithm and as we said earlier it's very similar to the Viterbi

32:27,32:34
algorithm the main difference is how i combine these these forward variables

32:34,32:39
and how we define them so instead of using Viterbi instead of using an actual soft max and a sum

32:39,32:46
would use would use something like a max a max and a sum or something like that

32:46,32:52
but otherwise the idea is the same and then the final score the score of p of y given x the sum over all possible

32:52,32:59
alignments remember that's what we wanted to compute in the first place is just given by the forward variable

32:59,33:05
alpha capital t capital u the number of input frames and the

33:05,33:11
number of output frames

33:11,33:19
so let's look at a let's look and see what that looks like more visually so this is a 2d graph it encodes

33:19,33:25
on the on the y on the vertical is the transcription c-a-t on the

33:25,33:31
horizontal is the number of is the input frames let's say there are five

33:31,33:38
and at each step we're going to kind of compute the forward variable or forward variables for that step

33:38,33:44
based on the four variables in the previous step so you can get a visual sense of how this algorithm is

33:44,33:49
proceeding so you know to start it's simple the forward variable

33:49,33:55
of length one for the output c is just the score of c given x one so

33:55,34:00
we have that you know we can read that directly out of the output of the network

34:00,34:07
so then at the next step i can compute both forward variables of length which

34:07,34:13
correspond to alignments of length two there's one for the alignment for out for ending in a and

34:13,34:20
one for ending in c and to compute these i just add in

34:20,34:26
the scores of p of a given the second frame and p of c given the second frame

34:26,34:35
for the first one and second one respectively so i keep proceeding in this fashion and i build up my forward variables and

34:35,34:42
over time i get to the end and i you know the alpha at the end

34:42,34:47
equals the actual soft max of the previous two time step alphas

34:47,34:54
plus the score of the kind of the trans the output at that at that level

34:54,35:02
so kind of alpha sub five cap alpha five with the output t

35:02,35:10
is the sum of the this actual soft max of alpha four of the two alpha fours with the score

35:10,35:15
of t given the fifth frame

35:15,35:22
so here's a question over here why are we doing this instead of the derby where are we doing this summation

35:22,35:27
of multiple paths rather than taking the most likely path

35:27,35:33
right it's a it's a good question actually and the answer is you could take the best

35:33,35:39
path you could do it this the simplest answer i can give is it works better to

35:39,35:45
do it this way rather than pick the best path initially

35:45,35:51
rather than pick the best path from this graph computing the sum over all possible paths leads to

35:51,35:58
better more accurate models in general i can give some intuition for why that's

35:58,36:03
the case and the intuition kind of is

36:03,36:11
that you know initially the model

36:11,36:17
does not know what a good alignment is between cat say cat

36:17,36:25
and the input sequence the five frames x1 through x5 if i if i use the Viterbi

36:25,36:30
algorithm to start then i'm basically saying well

36:30,36:37
i'm going to force the model to choose early on which alignment

36:37,36:43
to pick and hence which alignment increase the score for and i'll only

36:43,36:49
pick one and maybe it's not a good one maybe the initialization of my model

36:49,36:55
chooses some really funky degenerate alignment which it then tries to optimize the

36:55,37:02
score for that would be bad so rather than kind of

37:02,37:09
letting something like that happen which the Viterbi algorithm can we're gonna instead try to do we're

37:09,37:16
gonna instead sum over all possible alignments and that way the model has the freedom

37:16,37:23
to distribute the this the probability mass as it chooses between different

37:23,37:30
alignments and if given that freedom and enough data it turns out that it will

37:30,37:36
actually start to choose the right alignment so even though we give it the freedom to

37:36,37:43
increase the score over all possible alignments as we train the model it actually starts to

37:43,37:50
zero in on what is a good alignment and so you'll see that like one of these

37:50,37:55
paths as the model learns starts to get the most probability

37:55,38:01
and so the forward algorithm in a sense kind of becomes a Viterbi algorithm as we train even though we don't switch

38:01,38:07
explicitly as we train the model it starts to kind of choose the best path on its own

38:07,38:13
and give most of the mass to that path so you could do something like and this would probably work pretty well in

38:13,38:20
practice you could start with the forward algorithm and switch to the Viterbi algorithm after converging a little bit and

38:20,38:26
that would be fine but it's really kind of that early on figuring out which alignment to use

38:26,38:32
that's very important that's one intuition you can probably think of

38:32,38:38
other hypotheses for why this is a good thing to do any other questions yeah it makes sense

38:38,38:44
also last time in the homework we actually pre-trained the network for doing like some

38:44,38:49
classification in order to get some already like good starting points for

38:49,38:54
like recognition of the different phonemes or characters

38:54,39:01
right cool yeah makes sense good so

39:01,39:08
what Iâ€™ve described so far it's a fairly simple algorithm it's not ctc it's in its

39:08,39:14
entirety it's an algorithm that would that would work probably in practice

39:14,39:22
but doesn't work that well so people don't really use it for speech recognition that is and one of the reasons it doesn't

39:22,39:28
work that well for speech is because when you're when you give me a snippet

39:28,39:33
of audio that has human speech it's not only human speech there can be

39:33,39:39
frames like 20 millisecond frames which are silenced just nothing there can be noise stretches of noise

39:39,39:46
there can be laughter all sorts of things i don't want to force the model to output a token of my transcript

39:46,39:52
for every one of those frames and so you know

39:52,39:59
if i have silence say the third frame what can i put there instead of putting one of the letters from my transcription

39:59,40:05
and so ctc one of the things that it does differently than what Iâ€™ve

40:05,40:13
described so far is it has this notion of a garbage token or blank token and the blank token

40:13,40:20
just says this is there's nothing here basically or nothing that i care about at least so

40:20,40:25
you know if i transcribe the brent if i if i align the blank token to this third frame

40:25,40:31
you know after i produce my final predictions for each frame i'll just remove all those blank tokens because i

40:31,40:36
don't care about them and that will give me the final transcription

40:36,40:44
in ctc the blank token is actually optional so we don't have to use it ever but

40:44,40:49
we can use it the model can use it and it can use it one or more times in between

40:49,40:55
any of the outputs or before and before any of the outputs are after any adapters

40:55,41:02
so let's get a little familiar with kind of what this blank token buys us

41:02,41:12
so like i said the blank token is optional so say we have an input with five frames

41:12,41:18
and we have the output c-a-t cat as in before so this shows kind of a set

41:18,41:23
of three possible alignments the first alignment has the blank

41:23,41:29
token in the middle it's allowed because when i remove the blank and i collapse the

41:29,41:35
repeats the repeat t's i get cat back and that's good that's what i want the second alignment is also

41:35,41:41
allowed as we said the blank is optional there's no blank in that alignment that's okay we don't need to use the blank

41:41,41:48
when i collapse the a's and collapse the t's i get cat back which is what which is what we want and

41:48,41:54
and last the last alignment is also valid because it produces cat as well

41:54,42:01
for the same reasons what about this one is this alignment allowed c a t

42:01,42:09
blank t everyone think about this for a second is this does this alignment make sense

42:09,42:17
if i remove the blank am i gonna get the transcription that i want c-a-t

42:17,42:24
it does not make sense this alignment is not allowed and that's because what you know when

42:24,42:31
you remove the blank you're left with two t's not one so this would correspond to the transcription c-a-t-t

42:31,42:37
which is decidedly not c-a-t so it doesn't work for us

42:37,42:44
and this gets us into another subtle aspect of ctc which is that

42:44,42:49
if i have repeat tokens in my transcription

42:49,42:57
i must incorporate a blank between them otherwise there's no way to disambiguate

42:57,43:03
repeats from non-repeats so in general blank is optional between any

43:03,43:12
two letters of my output the model can always output a blank between any two letters or not as it so desires but when there

43:12,43:18
are repeat tokens consecutive repeats it must output at least one blank

43:18,43:25
and so you know if the blank were not there when we went to construct the final

43:25,43:30
transcription we just get f o d and not f o o d food which is what we want

43:30,43:36
i don't have a question here though so the double o in English it has its own phoneme right

43:36,43:44
which is going to be the o sound like food is not fold so how do you actually perform the

43:44,43:52
how do you connect the speech to the actual phonemes to the actual text that's not clear to me okay

43:52,43:58
good question so first the first answer is there is

43:58,44:03
no concept of a phoneme in this system just so you can just get rid of that

44:03,44:09
concept from being a unit of a modeling unit which we're going to use

44:09,44:14
explicitly when we decided to make our system more

44:14,44:20
end to end we got rid of this phoneme concept our model is predicting letters directly

44:20,44:28
so we don't really care about phonemes and as much as it you know

44:28,44:33
they're not you know they're not useful to us as an explicit modeling unit implicitly

44:33,44:38
the network may choose to kind of represent things internally as

44:38,44:45
volumes that's up to the network and probably it does do something like that

44:45,44:52
and maybe in some cases it would make sense to try to choose your letter representations instead of using letters

44:52,44:58
using kind of higher level tokens something like word

44:58,45:05
pieces or syllables which are more faithful to the sounds

45:05,45:10
that we make but in practice we just let the network figure out which letters to produce

45:10,45:17
from the data and it can figure that out on its own did i answer your question so i'm thinking you said each of those

45:17,45:24
x1 x2 x3 are taking 20 milliseconds right so in the word food there is no

45:24,45:30
i don't see how there is like a blank in between the o's like it's just the

45:30,45:39
same sound no of food no i don't see how it's like full space no right right

45:39,45:47
right yeah so yeah so the network has to kind of figure out that when there's when it's so okay so

45:47,45:54
Iâ€™ve glossed over a couple things here so first of all i'm showing that each letter corresponds

45:54,46:01
to one input but in practice the overlap is actually very large so even though

46:01,46:08
x1 and x2 differ by a few milliseconds for where they're selected like their

46:08,46:16
center point in the audio they overlap considerably and so to produce a given letter

46:16,46:24
the model actually has a very large window into the input i see okay not only that usually we

46:24,46:32
yeah so that's the main thing so we can start to figure out like oh this this looks like a sequence f-o-o

46:32,46:37
that Iâ€™ve you know this looks like that sequence and if i see that sequence i know i need to put a blank

46:37,46:42
a blank at this kind of position because it has access to enough context to do

46:42,46:50
that so it's mostly like learning to do it even though there's nothing in the audio

46:50,46:55
there's it really needs to rely on the context

46:55,47:00
that we give it in order to figure out to put a blank there it can't if you were to use just a very s thin

47:00,47:07
sliver of input it wouldn't work

47:07,47:12
okay so there is a question here which is sort of asking connections between what

47:12,47:18
you just explained with the blank with our homework so in our homework we enforce

47:18,47:25
a space and a break after each character so we had like

47:25,47:31
basically f blank oh blank oh blank d blank right instead here

47:31,47:38
you're mentioning you're saying that the these blanks are optional we don't have necessarily to go through the blank

47:38,47:43
right that's correct the blank is optional here the only time it's not is when

47:43,47:51
there's repeats in the output um so what was that the question or

47:51,47:57
yeah i guess more maybe it was more technical maybe we should be handling this on our side it was like more making

47:57,48:03
connection with the homework where we actually enforce a break our after every character i see

48:03,48:09
okay yeah here we don't enforce a break after every character so we do have a space character which

48:09,48:17
you know i didn't show here but we the model would output a space in between words so we can it can

48:17,48:23
we can ask the model to do word segmentation for us the blank is distinct from that it's

48:23,48:31
it's really to it can appear anywhere in the output and symbol it's optional

48:31,48:37
okay so move on yeah

48:37,48:45
so in ctc like we said the blank is optional so that means the recursion has three cases instead of just the

48:45,48:51
simple case that i showed earlier where you know you can compute the forward variable from

48:51,48:59
the previous two forward variables in ctc there's three cases so there's the simple case where

48:59,49:06
where the blank is in between two distinct letters in which case it's optional so you know

49:06,49:13
you can transition from you know you can transition from the previous letter at the previous time step you can

49:13,49:20
transition from the blank at the previous time step where you can transition from the current letter at the previous time step so basically

49:20,49:25
this is saying that i can i can compute my alignments of length t plus 1

49:25,49:31
from alignments of length t either by extending

49:31,49:37
the alignments all the alignments which ended in the previous letter of the output a extending all the alignments which ended

49:37,49:44
in blank or extending all the alignments which end in the current output b this is the first case where

49:44,49:52
the blank is optional the second case the is that the output

49:52,49:57
is not optional you know you have to output everything in your transcription at least once

49:57,50:04
so i'm not allowed to go from like blank to blank skipping outputs so i can't i

50:04,50:09
can't transition from alignments of length t which ended in blank

50:09,50:15
you know before the a two alignments of length t which end in

50:15,50:25
blank after the a i have to output an a in between so i'm only allowed to transition from these two previous notes

50:25,50:32
and the third case which looks like the second case in practice the math is the same is the blank

50:32,50:38
is not optional when there are repeats in the output so if i have a and a surrounding the

50:38,50:46
blank i'm not allowed to include alignments for the first a

50:46,50:55
i'm only allowed to include the blanks the alignment's ending blank and the alignments ending in in the second day

50:55,51:04
so those are the three cases for the ctc recursion when you combine those three cases and you can construct your forward

51:04,51:10
algorithm from those three cases and you know we've accomplished our original goal which

51:10,51:15
was to compute the score of the transcription given the

51:15,51:20
input and to do it efficiently and importantly to do it in a

51:20,51:26
differentiable way if you recall that all these scores are done

51:26,51:31
using our actual SoftMax actual soft max is some combination of logs and

51:31,51:37
exponentiation and addition and so everything is differentiable

51:37,51:42
we can differentiate through this graph traversal

51:42,51:47
and back propagate as usual get gradients with respect to parameters and

51:47,51:55
learn a model which optimizes the transcription given our input

51:55,52:01
so if you recall i showed you that graph earlier which was for the simple set of

52:01,52:09
alignments where you just kind of have a c one or more times in a one or more times and a t one or more times

52:09,52:16
you can draw the same graph for ctc it looks more complicated it is more complicated actually it's

52:16,52:22
definitely much less regular so it makes cdc more complex to implement and practice

52:22,52:28
but the graph but kind of viewing these viewing this

52:28,52:36
algorithm as a graph is actually quite useful and when you start to gain that skill of

52:36,52:41
of looking at these computations and envisioning kind of the graph

52:41,52:47
we can then start to think of them as operations on graphs and that will be

52:47,52:54
important for later on when we talk about graph transformer networks so i just want to step through this

52:54,53:01
graph very briefly just to kind of keep exercising that muscle of looking

53:01,53:06
at this as the set of alignments as a graph

53:06,53:12
so like we said we have an in a starting state which is the bold

53:12,53:18
state and in this graph there's two accepting states the two concentric circled states at the end

53:18,53:25
on every edge there's our label which is the is on the first self loop

53:25,53:31
would be the blank token and then slash the weight and here we don't have any weights we don't care about the weights

53:31,53:38
but in the future we will so what this graph encodes is you know

53:38,53:44
if you look at this zeroth state you can kind of output zero or more blanks the blank is

53:44,53:53
optional to start but you have to output a c and you can output more c's as you like or none

53:53,54:00
and once you've output your c's you have a choice you can either output your a directly transition from

54:00,54:06
one to three along the bottom edge or you can output a blank one or more

54:06,54:12
times transitioning from one to two and then two on the self loop and then output an a either way you have

54:12,54:18
to output the a you can't ever skip producing an a

54:18,54:23
but you can optionally choose to produce one or more blanks and so we keep going through this graph

54:23,54:29
and what we end up with is a set of possible alignments which include the optional blank

54:29,54:38
of arbitrary length for the transcription c18

54:38,54:45
that was that was the introduction to the ctc loss and how it works so to recap

54:45,54:52
we've gone over modern speech recognition how it's gone from kind of complex to end to end

54:52,54:57
we talked about the ctc loss which is one of the more commonly used not the only

54:57,55:04
loss functions used to train these more end-to-end speech systems and so now that we have these trained

55:04,55:11
models we know how to compute the score of a transcription given some input audio we know how to compute

55:11,55:16
its conditional probability we would like to be able to solve the

55:16,55:23
problem okay given some new input audio how do i find the transcription that's best according to the model that

55:23,55:31
Iâ€™ve trained and that's what decoding with beam search is going to do so like i said the

55:31,55:38
goal is we're given some input speech x we want to find the best transcription

55:38,55:43
so we assume we have two models we've got these models that have been already trained they're handed to us

55:43,55:49
one is the model which gives us the score of the transcript of any transcription given some input

55:49,55:56
some speech and the second is a language model which gives the score of just the

55:56,56:03
transcription not conditioned on speech so you know the goal of the language model is to assign high probability to

56:03,56:11
sequences of words or letters which are likely for the you know for human speech

56:11,56:16
and low probability otherwise so i want to talk a little bit about

56:16,56:24
where this language model came from and why we still use it so

56:24,56:31
the main reason we still use a language model is because it lets us it can be trained on a much

56:31,56:37
larger text corpus than what than what we can train our acoustic

56:37,56:43
model on so if you think about it for the acoustic model to train it we need we need these pairs right we need we

56:43,56:49
need so-called paired data or transcribe data we need audio and their corresponding transcriptions

56:49,56:54
and typically you know generating if you're if you're a company

56:54,57:01
like Facebook you can you can kind of pay people to transcribe like say public videos

57:01,57:07
and the speech in public videos but that's expensive like you know

57:07,57:13
paying someone to transcribe what's being said in a video is costly so that limits really the size of the

57:13,57:20
data sets that we can use that are paired where we have transcriptions but it's super easy to collect huge text

57:20,57:25
corpuses which don't have paired audio and just crawl the web

57:25,57:32
and so that lets us train a language model on a huge text corpus and then i can use that language model

57:32,57:38
to help figure out what the right transcription is you can imagine that if i produce a transcription which is

57:38,57:45
semantically or syntactically odd the language model should be able to say no don't do that instead favor this

57:45,57:51
other one so another really nice feature about the

57:51,57:59
language model is it lets us rapidly tune a system to a given application

57:59,58:06
or even a user so when you're when you're using say your phone and

58:06,58:12
you say like you know call x y and z you know call alfredo it turns out your

58:12,58:18
phone actually does something really sophisticated it constructs on the fly well

58:18,58:25
sort of a language model of all the names and contacts in your phone and then it uses that

58:25,58:33
language model to figure out you know what you said who you intended to call

58:33,58:38
because the names on different people's phones are so different and sometimes quite

58:38,58:46
quite idiot you know quite distinct or unusual that really that that language model

58:46,58:51
biasing really helps figure out which contact you said

58:51,58:56
that that feature would work much less well if we didn't use these kind of on-demand

58:56,59:02
user-specific language models so typically these language models

59:02,59:11
will be engram language models they'll just be based on counts of kind of co-occurrences of say three of length

59:11,59:16
three sequences of words sometimes length five it just depends on the use case

59:16,59:22
but more and more these days especially in research we've been gravitating towards things like

59:22,59:27
recurrent neural network based language models and even transformer based language models

59:27,59:33
but typically in in production systems these are still engram language models which have which

59:33,59:39
which are efficient and can be trained on you know gigabytes sometimes even

59:39,59:46
terabytes of text data okay so we said we we're given these two

59:46,59:55
models our acoustic model and our language model and we'd like to compute the transcription

59:55,1:00:02
which maximizes the sum of the score of these two models so you hand me some speech Iâ€™ve never seen it before i need to find a

1:00:02,1:00:07
transcription which maximizes the sum of the score of

1:00:07,1:00:12
these two models so you can imagine searching over the space of all possible transcriptions

1:00:12,1:00:18
each transcription i kind of feed it through both of these models i look at the sum of the scores and then i take the one which is the

1:00:18,1:00:24
best of course we can't do that explicitly because

1:00:24,1:00:31
there's a huge number of transcriptions so we need a way to do this efficiently

1:00:31,1:00:38
and in practice we're also not going to do it exactly but we're going to do it approximately so why star this this best transcription

1:00:38,1:00:45
isn't going to perfectly maximize these scores but it will approximately maximize them

1:00:45,1:00:53
we'll come pretty close

1:00:53,1:01:00
so the first thing you might think to try is okay maybe so okay so first of all

1:01:00,1:01:07
basically finding this best transcription boils down to a graph search i'm looking for the

1:01:07,1:01:13
lowest scoring path in in a graph and at each point

1:01:13,1:01:18
in my graph i can extend my note the node that i'm currently at

1:01:18,1:01:23
by all possible next letters so you can say at the first node

1:01:23,1:01:30
i can output any possible letter let's say our alphabet is abc for simplicity then

1:01:30,1:01:35
you know if i output an a at my first node

1:01:35,1:01:41
that will be the first letter in my transcription and the slash you know say that a on the

1:01:41,1:01:47
edge here is the first letter in the transcription the three is the score and then when i get to the next node i

1:01:47,1:01:53
can consider all possible extensions all possible letters for the second time step

1:01:53,1:01:59
so that would be abc for the second time step in their scores just a quick aside here i switched

1:01:59,1:02:05
from looking for the for the highest scoring to the lowest scoring so these should be interpreted as negative log

1:02:05,1:02:11
probabilities sorry if that's confusing but just make the switch in your head real quick we're

1:02:11,1:02:17
looking for the lowest scoring path in this graph

1:02:17,1:02:23
so the first thing you might think to try is just a greedy search right you might say okay at the first

1:02:23,1:02:29
time step i'll choose the letter which has the lowest score according to the model

1:02:29,1:02:34
and that in this case that would be c its score would be one

1:02:34,1:02:39
and and so that will be the first letter of my transcription

1:02:39,1:02:47
at the second step i will consider all possible extensions to the first letter c i'll look at those

1:02:47,1:02:53
scores i'll take the best one so i'm just looking at this local

1:02:53,1:02:58
set of possible extensions in this case the best one is a b with a score of two that gives me a

1:02:58,1:03:04
total score of three so my transcription is cb and it has a total score of three

1:03:04,1:03:12
at the third step i do the same thing i consider all possible extensions of cb i take the the one that has the

1:03:12,1:03:19
lowest score which is again a b with a score of 8 and that gives me a final score of 11.

1:03:19,1:03:26
and so the best path so far is a is a sequence cbb which has a has a score of 11.

1:03:26,1:03:33
and let's say that our input has has only three steps so we'd be done at

1:03:33,1:03:38
this point because our output we'd have processed the three steps of the input and we'd have produced

1:03:38,1:03:44
the output cvb but it turns out you know because of this greedy process i

1:03:44,1:03:50
actually missed a much better path there was this path

1:03:50,1:03:57
a b a which i missed because i didn't consider a as a possibility in

1:03:57,1:04:04
the beginning and so greedy really is

1:04:04,1:04:11
can quickly miss really good scoring paths especially if i

1:04:11,1:04:17
if you have kind of uncertainty early on when you're doing the search which you

1:04:17,1:04:25
often do so greedy is not going to work too well for us we want something better

1:04:25,1:04:32
so instead we're going to use a beam search and beam search is actually a very simple algorithm

1:04:32,1:04:38
and also very important to the speech recognition system and the way that it works is it's just

1:04:38,1:04:44
these two steps the first step is at any given stage in the algorithm i

1:04:44,1:04:50
will i have a set of candidates let's call that set of size

1:04:50,1:04:56
n so i have a set of n candidates current possible transcriptions

1:04:56,1:05:04
i'll consider extending each of those n candidates by all possibilities in my alphabet so abc

1:05:04,1:05:09
so then i will get you know n times the size of the alphabet candidates

1:05:09,1:05:15
i will in my second step of my algorithm i will sort all those new candidates by their

1:05:15,1:05:21
scores and i'll just chop off everything but the top end

1:05:21,1:05:28
and then i'll repeat this so the invariant is that when i start this loop i have a set of n

1:05:28,1:05:34
candidates and then when i repeat i always kind of have this

1:05:34,1:05:39
a set of size n so i consider extending that set then i truncate it to the best end that

1:05:39,1:05:49
i consider extending that truncate so on and so forth till Iâ€™ve consumed all that

1:05:49,1:05:55
so let's look at a quick example of how that looks

1:05:55,1:06:00
so say i start at the first node if n equals three meaning i'm going to

1:06:00,1:06:06
keep a set of three candidates at all times well then we'll just take all three of the first candidates

1:06:06,1:06:13
because our alphabet is also size three so we have a as the first possibility in our transcript b and c

1:06:13,1:06:20
and they have the corresponding scores and the notes at the second step i consider like we said we consider extending each node by

1:06:20,1:06:26
all possible letters so there are nine total

1:06:26,1:06:31
candidates i will look at the scores of all nine of those

1:06:31,1:06:36
candidates and i'll take the three best in this case the three best

1:06:36,1:06:43
are the three that we've highlighted here a b should score six c a as score four and c

1:06:43,1:06:50
c that's a score of five so that's my new and best list my new set of candidates and i just do

1:06:50,1:06:58
the same thing i extend those by all three possibilities each i sort those

1:06:58,1:07:05
by their scores i take the top three and i'm left with three more the three

1:07:05,1:07:10
newest set of candidates so let's say that at this point if my input has

1:07:10,1:07:16
length three i'm done Iâ€™ve looked at all three steps of the input

1:07:16,1:07:22
and Iâ€™ve got three candidates and what i'm going to return from this algorithm is just those

1:07:22,1:07:28
three candidates sorted by score we'll call that an n best and best list

1:07:28,1:07:36
and so the highest scoring one which will be the ultimate transcription that we'll use is ccb or rather the lowest scoring one

1:07:36,1:07:42
the best score and then we'll also return the other two just in case sometimes it's useful

1:07:42,1:07:48
we don't have to so that's beam search and then one thing

1:07:48,1:07:54
you might be wondering is where did these scores on the edges come from well the scores on the edges come from a

1:07:54,1:08:01
combination of the language model and the acoustic model so that's where we kind of integrate in

1:08:01,1:08:07
the two models that we were using that we were given earlier so each kind of step

1:08:07,1:08:14
we have to query the language model and the acoustic model to produce the score on a given arc

1:08:14,1:08:20
but otherwise this is the whole decoding process it's a beam search which keeps track of an end best

1:08:20,1:08:29
list and consumes the input when you're done you return the best path right

1:08:29,1:08:36
and so like we said we can use this beam search to approximately find the transcription

1:08:36,1:08:42
which is optimal under the acoustic model and the language model

1:08:42,1:08:48
and so that's how inference works it's quite straightforward at a high level in

1:08:48,1:08:53
practice there are lots of things one has to do

1:08:53,1:09:00
in the details to make it efficient especially because these language models and acoustic models can

1:09:00,1:09:08
get very large and require a lot of context to evaluate but the that's these are the main ideas

1:09:08,1:09:15
so we talked about hold on actually there is a question i missed it has there been

1:09:15,1:09:24
any research in differentiating the beam search so that we can directly optimize what we do in inference

1:09:24,1:09:31
yes there has and i

1:09:31,1:09:39
i myself have participated in such research so

1:09:39,1:09:45
basically i mean the answer is yes there you know there's

1:09:45,1:09:51
lots of there's a couple of good reasons to try to make a beam search differentiable

1:09:51,1:09:57
one reason is because then we can make training time conditions more

1:09:57,1:10:03
similar to test time conditions so you know you notice there's this mismatch between training time and test

1:10:03,1:10:09
time right we're at training time we're using ctc and marginalizing over all possible

1:10:09,1:10:15
alignments at test time we're doing this beam search using a language model these are two

1:10:15,1:10:21
very different processes if i make my beam search differentiable

1:10:21,1:10:27
i can actually use it at training time as the loss function directly um and

1:10:27,1:10:35
so that's a good thing to try to make those two consistent with one another um to some extent you could say that a

1:10:35,1:10:41
graph transformer network which you are about to talk about is actually an attempt that differentiating

1:10:41,1:10:46
through a beam search even though the beam search itself is not differentiable that's exactly right basically what i

1:10:46,1:10:53
was going to say next so that's one of the motivations for graph transformer networks in the first place

1:10:53,1:11:00
no i'm glad you said the same thing okay so let's figure out what these gaps

1:11:00,1:11:07
transformer networks are and how also they differentiate from the other graph networks we're going to be learning soon

1:11:07,1:11:14
right right okay so yes so let's talk about graph transformer networks in the remaining time

1:11:14,1:11:20
and so i'm going to start [Music] i'm going to start well just

1:11:20,1:11:27
reintroducing this data structure that we talked about a little bit and then i'd like to do kind of like a

1:11:27,1:11:33
high-level discussion of graph transformer networks a little bit on the history where they came from what they're

1:11:33,1:11:39
used for and then go into some of the low-level details

1:11:39,1:11:46
of how we can construct these graphs operations on them and

1:11:46,1:11:52
and then a couple of examples so just to orient everyone to start

1:11:52,1:12:00
though the graph transformer networks at least the ones that that we're using to do research with at Facebook are

1:12:00,1:12:06
built on top of this data structure which we call a weighted finite state

1:12:06,1:12:11
automaton and this is the same graph i showed you earlier it encodes the set of

1:12:11,1:12:17
alignments for why the transcription cat

1:12:17,1:12:23
and really what gtns are there a way to use

1:12:23,1:12:29
these graphs to perform operations on these graphs coupled with automatic or with

1:12:29,1:12:37
differentiation in our case automatic differentiation through those operations so you can think of gtns as

1:12:37,1:12:43
instead of tensors you have graphs like this graph that Iâ€™ve shown you

1:12:43,1:12:50
and instead of matrix multiply convolution and you know pointwise

1:12:50,1:12:56
operations you have different and new and interesting graph operations

1:12:56,1:13:01
and just like they're just like the operations that you can do on tensors like matrix

1:13:01,1:13:07
multiply and convolution those graph operations are actually differentiable and when i say differentiable

1:13:07,1:13:12
i mean you can differentiate the output of the operation with respect to

1:13:12,1:13:18
the input specifically the arcs of the input graphs the weights on the arcs of the

1:13:18,1:13:24
input graphs we'll talk a little bit about what these operations are

1:13:24,1:13:29
and what the weights are and where they come from but first i want to talk a bit about the

1:13:29,1:13:36
history of graph transformer networks so you know as i said earlier

1:13:36,1:13:42
these models were developed by people like Leon but two at fair

1:13:42,1:13:49
Yann and others at a t in in the early 90s and john correct me

1:13:49,1:13:54
if i'm getting any of this history wrong please um and then one of the first

1:13:54,1:14:01
applications was in in a state-of-the-art automatic check

1:14:01,1:14:07
reading system which was actually deployed using gtns and deployed widely to the best of my

1:14:07,1:14:13
knowledge so it's kind of one of the early success stories of ai actually being used in practice

1:14:13,1:14:21
and i find that very cool in the early 90s you know these things were already deployed

1:14:21,1:14:27
so if you go back and you know you kind of follow the

1:14:27,1:14:33
explosion of deep learning over the past decade or so one of the papers that gets

1:14:33,1:14:40
most cited is this is this paper from yawning company which kind of introduces

1:14:40,1:14:47
convolutional networks and even like modern deep learning as you know building blocks constructed from these

1:14:47,1:14:53
convolutional networks for image processing this is actually a long paper it's like

1:14:53,1:14:59
40 some pages and most of the convolutional network

1:14:59,1:15:07
specific part of it is actually just the first half like the first even the first third 16 pages but it turns out

1:15:07,1:15:13
you know the whole second half of this paper is about graph transformer

1:15:13,1:15:18
networks and you know it's less frequently read unfortunately because it's actually

1:15:18,1:15:25
very interesting i'd say even more interesting in the first half and so do yourself a favor and read

1:15:25,1:15:32
it if you have time um but also to me kind of in general this this this paper is remarkably prescient in

1:15:32,1:15:38
terms of how much of what we do in modern deep learning it was already doing albeit at a smaller

1:15:38,1:15:44
scale and hopefully and it it's prescient of what we will be

1:15:44,1:15:51
doing with gtns which we're starting to now and i'm hoping people will adopt more

1:15:51,1:15:56
in in the future so on the right here you can kind of see a little figure

1:15:56,1:16:02
which really summarizes that taken from their paper which really summarizes

1:16:02,1:16:07
the parallel between graph transformer networks and

1:16:07,1:16:13
at this end i'm going to call it traditional deep learning which is kind of funny because there's nothing yet traditional about deep

1:16:13,1:16:21
learning but maybe graph transformer networks will make it make them traditional

1:16:21,1:16:28
so really the main distinction is you're operating on a different data structure which is a graph

1:16:28,1:16:35
and the operations will of course be different so to draw this parallel even further

1:16:35,1:16:41
with deep learning with neural networks our core data structure is a tensor

1:16:41,1:16:46
it can be like a 1d or nd tensor and

1:16:46,1:16:53
and with gtn's our core data structure is a graph typically some kind of like weighted finite state automaton like the one i showed earlier

1:16:53,1:16:59
although there are like there are variations of this graph data structure i'll give

1:16:59,1:17:05
you another variation later and the operations that we do they all have nice parallels

1:17:05,1:17:11
so in matrix multiplication the parallel in gtn's would be something like the composition or

1:17:11,1:17:18
intersection operation of two graphs the reduction operations they're parallel would be shortest distance

1:17:18,1:17:23
operations which include forward the forward algorithm and the Viterbi algorithm but on general graphs

1:17:23,1:17:28
instead of these very finely structured graphs that we we've been talking about

1:17:28,1:17:34
and similarly there's unity in binary operations which take a single

1:17:34,1:17:43
graph or two graphs and compute a new graph there's parallels there as well

1:17:43,1:17:50
so where are these graphs used today because they are actually used today

1:17:50,1:17:57
and they've been used for a while especially in speech recognition so you know wfst's wfsa's weighted

1:17:57,1:18:03
finite state acceptors are not new they've been used a lot and are currently used a lot

1:18:03,1:18:12
but the main distinction between how they're used today and what were you know what Yann and

1:18:12,1:18:18
Leon we're trying to do with graph transformer networks what we're trying to do is that they're only used at inference

1:18:18,1:18:26
today you know you can kind of view your model as its output as a graph or

1:18:26,1:18:31
the model itself as a graph but if you only use them at decoding well

1:18:31,1:18:36
you're really limiting what you can do with these models because they can be used

1:18:36,1:18:41
at training time as well and this this is one of the things we were hinting at earlier which is

1:18:41,1:18:47
if you use them at training time you can start to bridge the gap between what we're doing at decoding and what

1:18:47,1:18:53
we're doing at training such as making a beam search available to us

1:18:53,1:19:01
at training at training time so more concretely why are we interested

1:19:01,1:19:08
in in gtms why are we interested in in these finite state acceptors that can be with

1:19:08,1:19:15
with automatic differentiation well for one it's much easier to encode

1:19:15,1:19:22
knowledge about the world in one of these graphs than it is in a generic tensor like if i have some

1:19:22,1:19:30
knowledge that a word consists of letters or word pieces it's not obvious how do

1:19:30,1:19:36
you encode that in a tensor but you can encode that in a graph quite easily actually

1:19:36,1:19:43
if i have some knowledge about the set of alignments that should be should be

1:19:43,1:19:49
allowed by a model such as like a blank is optional i can encode that in a graph as i showed

1:19:49,1:19:54
you earlier but encoding it in a tensor it's kind of not clear how to do that

1:19:54,1:20:02
right so it's much easier to encode priors in these graphs

1:20:02,1:20:08
and then the second reason is basically what we're saying earlier if bridge using these graphs lets us

1:20:08,1:20:14
bridge and bring together training time and test time conditions which avoid

1:20:14,1:20:21
the common issues that result when we treat these two things as separate processes

1:20:21,1:20:28
and then and the third which is really one of my favorite aspects of

1:20:28,1:20:34
this approach is that it facilitates research so when you separate data

1:20:34,1:20:42
from code really anytime you do this you make it much easier to

1:20:42,1:20:48
develop so when we in our case the graph will be the data and the code will be

1:20:48,1:20:53
the operations on graphs when we treat those two things as separate rather than trying to encode

1:20:53,1:21:00
the data in the code itself the graph and the code itself makes it much easier to explore

1:21:00,1:21:05
different ideas because then all we have to do is change the graph

1:21:05,1:21:17
and we can all of a sudden we have a new and interesting algorithm i'll give an example of that later

1:21:17,1:21:22
so it turns out a lot of sequence criteria like connectionist temporal

1:21:22,1:21:28
classification ctc can be specified as the difference between the forward score of two

1:21:28,1:21:36
two graphs two weighted finite state acceptors and so you know the two graphs in

1:21:36,1:21:42
particular one is a function of both the output and the input we'll call that the graph a so it's

1:21:42,1:21:48
constrained by the transcription as well as the input and one is just the function of the

1:21:48,1:21:55
input call that the graph z and the graph z kind of serves as the normalization graph

1:21:55,1:22:00
so you know what we're trying to do intuitively is the graph which is constrained by the

1:22:00,1:22:06
target y we'd like to increase its score because it's it encodes all the paths

1:22:06,1:22:13
that we like that we think are good and the and the graph z which is not constrained by the target

1:22:13,1:22:19
encodes all possible paths including the ones that we think are good but also many more

1:22:19,1:22:25
and so we want to decrease the score of those paths because

1:22:25,1:22:32
then you know relatively speaking the score of the ones we care about will be higher and so this actually you know this

1:22:32,1:22:38
this this kind of intuition should sound familiar from things like energy based models which i think you all learned about

1:22:38,1:22:44
this is one way of combining graphs and you know computing criteria

1:22:44,1:22:50
it's not the only way um but this would be like you know using a soft max in your denominator

1:22:50,1:22:56
where you code all possibilities in the denominator and then you have just the ground truth and the

1:22:56,1:23:04
numerator so like i said there are many criteria

1:23:04,1:23:10
we can specify with these graphs including connections temporal classification as well as others that are commonly used

1:23:10,1:23:16
in speech recognition as well as also many more especially

1:23:16,1:23:21
especially if we you know if we consider other applications other than speech

1:23:21,1:23:28
so you know just to as a little teaser if you if you start to implement things with in this

1:23:28,1:23:34
framework of using graphs so say you take ctc you look at some common implementations

1:23:34,1:23:40
there's an implementation called warp ctc there's an implementation from us from a

1:23:40,1:23:45
speech recognition system at fair there's an implementation in pi torch

1:23:45,1:23:53
if you look at the number of lines of code of their custom ctc implementations it's in the thousands and if you

1:23:53,1:23:58
implement the same thing using graphs it only takes about 30

1:23:58,1:24:04
lines of code of course the work is still being done it's just being done in a more generic

1:24:04,1:24:09
and in a way which it can be applied to lots of different algorithms so

1:24:09,1:24:15
the code itself is now in the operations which are general and then to construct ctc i only need

1:24:15,1:24:23
to string together a few of those operations which is why it's so much simpler and also the same graphs

1:24:23,1:24:29
can be used at decoding i no longer have to re-implement or implement a custom

1:24:29,1:24:37
separate decoding cycle so it's a really big win in terms of development which translates in terms of

1:24:37,1:24:43
what the things that i can quickly explore in research okay so that was that was like a high

1:24:43,1:24:49
level discussion of graph transformer networks weighted finite state acceptors and so on

1:24:49,1:24:55
i'm going to talk now about some of the operations some of these graphs some of the

1:24:55,1:25:05
operations we can do on them and then and then get into a couple of examples

1:25:05,1:25:11
so here's a very simple graph we said earlier the bold state at the

1:25:11,1:25:17
beginning is the start state the accepting state is the is the one of concentric circles at the end

1:25:17,1:25:24
and so this graph will say it recognizes two sequences the first is a and the second is ba

1:25:24,1:25:30
so you know the first is a from the zero to two and the a from two to one and the second is

1:25:30,1:25:36
b from zero to two and a from two to one and the score you just read off the weights of each edge and

1:25:36,1:25:41
sum them up so the score of the a would be zero plus two which is two and the score

1:25:41,1:25:47
of ba is one plus two which is three so this graph in summary recognizes two

1:25:47,1:25:58
sequences and we can get the score of those sequences as the sum of the weights on the edges

1:25:58,1:26:04
so we call that this kind of graph an acceptor because it accepts sequences there's another kind of graph

1:26:04,1:26:11
which we call a transducer because it maps input sequences to output sequences

1:26:11,1:26:17
very similar concept the main difference is instead of having a just label on each edge we'll have an

1:26:17,1:26:24
input label colon and output label slash a weight and so the

1:26:24,1:26:30
the input label basically is the input and then the output label

1:26:30,1:26:36
corresponding output label is what it maps to so so this graph we would say it transduces

1:26:36,1:26:42
two sequences the first sequence it transduces a b to x z because the a maps to x and

1:26:42,1:26:48
then b maps to z and as in the first graph we just get the weights by summing the weights

1:26:48,1:26:55
on the edges so that's a transducer map sequences to sequences instead of

1:26:55,1:27:00
just accepting sequences so there's some different types of

1:27:00,1:27:07
graphs that we can have you know different structures cycles are allowed you can kind of go from zero to one to two back

1:27:07,1:27:13
to zero that's fine self loops are allowed

1:27:13,1:27:19
not all operations support self loops and cycles but in general we allow these you can have multiple start nodes

1:27:19,1:27:26
so the zero and the one here are both bolded circles they're both start nodes you can have multiple accept nodes

1:27:26,1:27:32
the three and the four states here both except nodes that's fine you'll find different flavors of this in

1:27:32,1:27:37
different implementations but in general these things are allowed

1:27:37,1:27:43
one of the more subtle and more useful

1:27:43,1:27:50
components or properties of or you know features of these graphs are epsilon transitions

1:27:50,1:27:57
and so these are really subtle take some getting used to but the basic idea is

1:27:57,1:28:04
epsilon is synonymous with like nothing so this graph the easiest way to think

1:28:04,1:28:10
about it is this graph accepts two sequences one is a b and the other is just b

1:28:10,1:28:15
because i can transition from zero to one without accepting without you know using

1:28:15,1:28:21
any tokens just by transitioning to epsilon and then and then i have to use the b

1:28:21,1:28:27
token to get from one to two so this graph accepts two sequences the a b and then just the b

1:28:27,1:28:36
because the epsilon says i can make that transition without consuming any tokens

1:28:36,1:28:44
and just as in just as in acceptors epsilons are allowed in transducers so this

1:28:44,1:28:50
graph transduces the sequence a b a for example by following the self loop

1:28:50,1:28:57
then the arc from 0 to 1 and the arc from 1 to 2. but the output would just be an x

1:28:57,1:29:03
because the output on the first two edges are epsilons and that that corresponds to nothing

1:29:03,1:29:10
so the final output is just an x and so you see what the epsilon buys us here is the ability

1:29:10,1:29:18
to map variable length inputs to variable length outputs so now

1:29:18,1:29:23
instead of aba mapping to something of length three i can actually map aba to something of

1:29:23,1:29:29
length one and i can also have epsilons on the input so i can map variable you know shorter inputs to longer outputs for

1:29:29,1:29:35
example okay so let's talk about a few different operations i'm just going to touch on a

1:29:35,1:29:41
couple of them definitely this is not comprehensive um but just to get a flavor so there's some

1:29:41,1:29:47
some very simple operations we can do like union and there's a few more

1:29:47,1:29:54
complex more complicated ones which are actually the main workhorses

1:29:54,1:30:02
so the union graph the union of graphs is the graph which you know it accepts

1:30:02,1:30:08
all paths which are accepted by any of the input graphs so like g1 g2 g3 are the input graphs on

1:30:08,1:30:14
the left they each accept some number of sequences

1:30:14,1:30:20
the output graph on the right which actually kind of looks the same but the distinction is it's a single data structure

1:30:20,1:30:25
instead of three separate data structures it's a single data single graph with three start nodes instead of three

1:30:25,1:30:31
graphs each with a start node is the union of those three graphs you see it's very simple to construct this

1:30:31,1:30:36
graph because i just kind of you know stitch them all together by making a new graph

1:30:36,1:30:42
which has three accepting nodes or three star nodes and it recognizes all

1:30:42,1:30:49
the sequences that are recognized by the inputs input graphs

1:30:49,1:30:56
there's another operation called cleaning closure which computes the closure of an

1:30:56,1:31:05
input graph so the closure is any number so if the input graph

1:31:05,1:31:11
accepts a sequence the closure of the input graph accepts one or more or zero or more

1:31:11,1:31:18
repetitions of that sequence so our input graph here accepts aba the closed graph accepts

1:31:18,1:31:25
you know the empty sequence or one or more copies of abn so fairly straightforward the way the

1:31:25,1:31:31
way that you can construct this graph and it can be done very efficiently is just by stitching together

1:31:31,1:31:38
the output the accepting node to the input node with an epsilon transition and making

1:31:38,1:31:44
the input node accepting to allow for rep accepting the empty string

1:31:44,1:31:51
okay so this next operation intersection intersect is more sophisticated i'm

1:31:51,1:31:57
actually going to go through how to how to do it because i think it's one it's the it's perhaps the most

1:31:57,1:32:05
important operation it's kind of like matrix multiply or convolution for wfsts or in wfsas so

1:32:05,1:32:12
it's good to get familiar with this one the idea of what it's computing is

1:32:12,1:32:17
straightforward though so if i have two input graphs and i

1:32:17,1:32:23
wanted to compute their intersection that means i want the graph which

1:32:23,1:32:29
encodes any sequence which is accepted which accepts any sequence which is accepted

1:32:29,1:32:34
by both input graphs so it's just like just like intersection and union

1:32:34,1:32:41
of sets the intersection of two graphs is the graph which accepts any sequence

1:32:41,1:32:46
which is accepted by both input graphs or any sequence yeah any sequences which

1:32:46,1:32:52
is accepted by both input graphs and the weight of those sequences

1:32:52,1:32:59
will be the sum of the weights from the input graphs so like if graph

1:32:59,1:33:06
one accepts a sequence x and graph two accepts the sequence x then the output the intersected graph

1:33:06,1:33:12
will also accept that sequence x and the weight will be the sum of the weights

1:33:12,1:33:23
of the two and the weights that the two input graphs assigned to x

1:33:23,1:33:29
so how does this operation work let's say we have these two input graphs i want to compute their intersection

1:33:29,1:33:39
so you know you can kind of stare at this and say like okay well the graph on the right

1:33:39,1:33:46
accepts the sequence a b the graph on the left also accepts the sequence a b

1:33:46,1:33:52
so that should be in the intersection is there anything else i don't think

1:33:52,1:33:57
so i think that's it so the intersection is really going to be a simple graph which

1:33:57,1:34:03
which accepts only the sequence a b so we're looking for like a graph with three nodes

1:34:03,1:34:08
where you know in between the first two it's a transition on a and between the second

1:34:08,1:34:15
two it's a transition on b so this is really simple you can kind of get the answer in your head just by looking at these two in practice

1:34:15,1:34:21
when the graphs get more complicated the intersection becomes you can't just compute it in your head

1:34:21,1:34:28
like that not even close but let's actually go through the algorithm to see how it works for these

1:34:28,1:34:33
two so we start by looking by considering the set of starting states

1:34:33,1:34:41
in both graphs and we construct in our intersected graph the combined starting state

1:34:41,1:34:47
so our intersected graph will have a starting state which is you know the combined starting state from the first two graphs

1:34:47,1:34:55
and then at each of those starting states we explore all possible outgoing transitions

1:34:55,1:35:02
and we ask the question do these transitions have the same label so we might consider

1:35:02,1:35:08
the outgoing transition on a that does have the same label so that

1:35:08,1:35:13
edge will add to our intersected graph at least our current hypothesis for the

1:35:13,1:35:19
intersected graph and we'll also add the node pair

1:35:19,1:35:26
which it leads to in each of the input graphs so in the left input graph it leads to 0

1:35:26,1:35:31
on the soft loop on the in the right it leads to a one so we'll construct this new state

1:35:31,1:35:37
in our intersected graph which is zero comma one which just encodes the kind of combined

1:35:37,1:35:43
state zero one from zero and one from the two inputs

1:35:43,1:35:48
so then i keep exploring the pair of all possible

1:35:48,1:35:55
arcs leading out from the two states in the input graph so i look at the pair

1:35:55,1:36:00
bb those two have the same label they match so i add

1:36:00,1:36:07
that arc to my input graph it's my intersected graph and they lead to a new state in my

1:36:07,1:36:13
intersected graph one that i haven't seen before which is which is the pair one the

1:36:13,1:36:20
combined state one then i consider the c the outgoing

1:36:20,1:36:25
transition from the zeroth state in the second graph there's no match in the first graph

1:36:25,1:36:32
the first graph doesn't have an outgoing transition on c so that edge gets dropped we don't use it

1:36:32,1:36:39
okay and then once i'm done exploring these this pair of states i move on to

1:36:39,1:36:44
the next pair that Iâ€™ve added to my intersected graph so that would be the pair

1:36:44,1:36:50
0 and 1. and i asked the same question what are

1:36:50,1:36:57
the outgoing arcs outgoing transitions from these two states which match

1:36:57,1:37:03
and so i see they match on the a so i add that a as an outgoing transition in the intersected graph

1:37:03,1:37:08
and i actually have reached a new node in my intersected graph which is

1:37:08,1:37:15
the combined state 0 comma 2. so i add that new node as well

1:37:15,1:37:22
i look at the b the b matches so i add the b is an outgoing state

1:37:22,1:37:29
or an outgoing transition in the intersected graph and actually something interesting has happened here which is that the

1:37:29,1:37:34
the b transitions if i follow them in the two input graphs

1:37:34,1:37:41
and led to a state which is accepting in both input graphs and so that means

1:37:41,1:37:46
it should be an accepting state in the intersected graph and Iâ€™ve marked it we've marked it as

1:37:46,1:37:55
such so that that will now be we now have an accepting state in our intersected graph

1:37:55,1:38:02
again there's no match for the c so that gets ignored okay now explore the next new the next

1:38:02,1:38:08
state that i added to my intersected graph which was the combined state one turns out this state is a dead end

1:38:08,1:38:13
there's no way to leave this state in the two input graphs which have the

1:38:13,1:38:19
same label so we've added this dead end path to our intersected graph we can just remove it

1:38:19,1:38:24
because there's no use in in having it so we remove it

1:38:24,1:38:30
and then we explore the next state that we added to our intersected graph which was the zero to the combined zero

1:38:30,1:38:36
two state again this state is a dead end there's no way to leave this state

1:38:36,1:38:41
on arcs which have the same label in our input graphs

1:38:41,1:38:47
so we just remove that state and then we look at the last state we

1:38:47,1:38:54
added to our intersected graph again there's no arcs to explore here it is a dead end

1:38:54,1:38:59
but since it's an accepting state we don't remove it we keep it because it's an accepting

1:38:59,1:39:04
state and there's no more arcs to explore and there's no more new nodes

1:39:04,1:39:12
there's no unexplored nodes in our intersected graph so at this point we're done we've computed the intersected graph and

1:39:12,1:39:18
lo and behold it is exactly what we wanted it encodes the sequence a b

1:39:18,1:39:31
and the corresponding score is just the sum of the scores from the input graphs

1:39:31,1:39:36
so that's how intersect works and intersect is you know if you didn't get it on that first pass

1:39:36,1:39:42
which is totally reasonable because it's kind of a sophisticated algorithm

1:39:42,1:39:49
it is it is one of the most if not the most important operations to to gtns and

1:39:49,1:39:57
and the differentiable wfsas and so you know kind of encourage you to go through some examples and just like

1:39:57,1:40:04
work out work through the steps of this algorithm you start to get an intuition there is a question about how do you

1:40:04,1:40:11
deal with the numerical values alongside a or b while performing intersection of graphs

1:40:11,1:40:16
right so let's look at a quick example so like early on when i added this

1:40:16,1:40:22
a transition i add the so this isn't a great example sorry

1:40:22,1:40:29
because the first graph has only zeros but when i when i added the new arc in this intersected graph

1:40:29,1:40:36
i just added the weight for that new arc is the sum of the weights on the arcs that

1:40:36,1:40:43
i see from the input graphs so it's 0.4 that's zero plus one but if it had been like one from the

1:40:43,1:40:48
input graph it'd be 1.4

1:40:48,1:40:53
okay so here's another example of

1:40:53,1:40:58
intersection which Iâ€™ve included in if you guys go back and refer to the slides you can

1:40:58,1:41:04
kind of work through work through how you construct this this

1:41:04,1:41:10
intersection graph from the two inputs so composition is basically the same

1:41:10,1:41:15
thing as intersection but the distinction is that

1:41:15,1:41:23
instead of operating on acceptors remember acceptors accept the sequence it operates on transducers which are

1:41:23,1:41:32
the graphs which map one sequence to another sequence and so instead of looking for

1:41:32,1:41:38
the paths which ex which match on what they accept we're going to match instead on the

1:41:38,1:41:44
inner sequence of the two graphs so

1:41:44,1:41:50
what i mean by that is if the input graph transduces x to y

1:41:50,1:41:59
and the output and the second input graph transduces y to z then we want our composed graph

1:41:59,1:42:06
to transduce x to z it will match on the y kind of splice it

1:42:06,1:42:12
out and then transduce from x to z and that's what we want our composed

1:42:12,1:42:18
graph to do the scores will be the sum of the scores

1:42:18,1:42:24
of the paths and input graph just like an intersect and in fact the algorithm which you use

1:42:24,1:42:31
for this is virtually the same instead of matching on input labels you just match on those inner labels instead

1:42:31,1:42:38
so i'm not going to go through it again um but here's analogous example

1:42:38,1:42:44
which you can kind of work through which shows you know if i'm given the input graph g1

1:42:44,1:42:51
and g2 i can construct their composition all right the main thing about

1:42:51,1:42:57
composition is it lets us map from different domains so like say i have i

1:42:57,1:43:04
have a graph which maps from say letters to words and i have another

1:43:04,1:43:10
graph which maps from words to sentences or phrases if i compose those two

1:43:10,1:43:18
graphs my composed graph will map from letters to phrases that can be a powerful concept kind of

1:43:18,1:43:27
composing these graphs through multiple hierarchies of representation

1:43:27,1:43:34
so just as in we had our forward algorithm with ctc

1:43:34,1:43:40
we have a general forward algorithm on these graphs it's virtually the same algorithm but

1:43:40,1:43:45
instead of kind of having a fixed number of inputs at each node we

1:43:45,1:43:53
just take all of the possible inputs and instead of having just one output at

1:43:53,1:43:58
each instead of the node being used for a fixed number of outputs it can be used for arbitrary number of outputs

1:43:58,1:44:05
but the way we kind of combine scores at each node is the same the forward algorithm does assume that

1:44:05,1:44:12
the graph is a dag that it's directed and it doesn't have cycles

1:44:12,1:44:20
so so this forward algorithm what it's doing is really it's giving us a way of computing efficiently the sum

1:44:20,1:44:25
of the scores of all the paths represented by a graph

1:44:25,1:44:31
so that's a useful operation it kind of lets us say like

1:44:31,1:44:39
does this you know kind of what's the score of this graph overall so for this example

1:44:39,1:44:45
you can compute the forward score you can do it explicitly

1:44:45,1:44:51
by delaying all possible paths this graph has three sequences which it accepts

1:44:51,1:44:57
aca going from zero to one to two to three the one state is also accepting or

1:44:57,1:45:04
starting here so we can start at one and so the path ca is also

1:45:04,1:45:10
allowed and then the path ba so the forward the forward score of this will be the

1:45:10,1:45:15
sum of all the actual soft max of all

1:45:15,1:45:22
of the scores of those paths

1:45:22,1:45:28
so as we said earlier we can we can kind of construct sequence

1:45:28,1:45:35
criteria we can construct loss functions from these graphs so you know if you remember we had

1:45:35,1:45:42
the graph the c-a-t graph from earlier what i showed you when we were kind

1:45:42,1:45:47
of looking at alignments for ctc we'll say that we have a very similar graph say our target instead of c a t

1:45:47,1:45:52
is just a b as the graph on the upper left

1:45:52,1:45:59
so that encodes that graph on the upper left just encodes the set of allowed alignments

1:45:59,1:46:05
of the sequence a b then the graph on the right would be something like

1:46:05,1:46:12
the set of all possible sequences of length four

1:46:12,1:46:18
which are which you know just the set of all

1:46:18,1:46:26
possible sequences of length four and the alphabet being abc

1:46:26,1:46:32
the way to think about that graph on the upper right the emissions graph is that at each kind of in between

1:46:32,1:46:40
each two node each two nodes you have you have a set of logits from your network a distribution

1:46:40,1:46:47
or you know a normalized distribution of over your alphabet at that time step

1:46:47,1:46:52
so that's encoded by the weights and then if i intersect these two graphs

1:46:52,1:46:58
i compute this target graph this target constrain graph which i'm

1:46:58,1:47:07
calling a and it represents all possible alignments of length 4 for the sequence a b

1:47:07,1:47:15
and if i then do the forward score of this graph the target constrain graph that gives me

1:47:15,1:47:23
kind of the sum over all possible alignments and then i can also normalize by

1:47:23,1:47:29
the sum of all possible unconstrained alignments so i have you know in summary i have the

1:47:29,1:47:34
sum of all possible alignments for the target that i care about which i want to increase the score

1:47:34,1:47:41
up and i have the sum for all possible sequences which are not constrained by

1:47:41,1:47:48
the target and i want to decrease the score of those and so i take the difference of these two things in log space

1:47:48,1:47:53
so that would be how we do a very simple kind of sequence loss function using these graphs

1:47:53,1:47:59
this isn't ctc but it's approaching ctc

1:47:59,1:48:05
okay let's get familiar with a little bit of code so i'm almost done here i'm going to give a couple of examples in code and then

1:48:05,1:48:11
and then we'll be finished but if there's any questions you know feel free to jump in

1:48:11,1:48:17
so just to give you a flavor for so we

1:48:17,1:48:22
have this framework called gtn which actually lets us construct

1:48:22,1:48:28
these graphs and then and then compute on them do the operations and then do automatic differentiation so

1:48:28,1:48:36
that we can learn using them so to give you a flavor for how to construct such a graph

1:48:36,1:48:42
in this framework i make my graph initially so you can see the firs the first line after importing the

1:48:42,1:48:51
framework for this graph i don't want a gradient so i just specified that i don't want a gradient in the parameter calc gradient

1:48:51,1:48:56
i can add nodes to the graph and designate whether or not they should be starting or accepting

1:48:56,1:49:03
so i add the zeroth node and make it a start node the middle node is not the final node isn't something that

1:49:03,1:49:08
and then i add arcs and the way i add an arc is i just specify the source node the destination node

1:49:08,1:49:14
and the label and in some cases the weight if i have a weight and then we can draw

1:49:14,1:49:19
these graphs and this this graph was drawn using exactly this

1:49:19,1:49:25
this function by you know there's a utility that lets

1:49:25,1:49:30
us draw and i have to specify the label map so i know kind of like what the integer

1:49:30,1:49:36
labels correspond to

1:49:36,1:49:42
another way to make so there's helper functions to make graphs from kind of arrays or tensors so

1:49:42,1:49:49
you know if you take the output of a network as a as a 2d array of logits where

1:49:49,1:49:55
you know you have four time steps in each time step you have three logits

1:49:55,1:50:02
so that's that NumPy array here the emissions array i can make what we call linear graph

1:50:02,1:50:07
using the logits for each time step

1:50:07,1:50:13
and that graph looks like the graph you see below that's the emissions graph it's just in codes in between each two

1:50:13,1:50:20
nodes the set of logits for that time step and for this graph we want a gradient so calculate should be true

1:50:20,1:50:27
and i set the weights i make the graph it has the right structure and then i set the weights based on the

1:50:27,1:50:32
array the NumPy array in this case

1:50:32,1:50:40
so now i can compute that loss function that i was showing you earlier in gtn this is what the actual code looks like i'm given my emissions

1:50:40,1:50:47
and the target graph these are two graphs i compute their intersection to get the

1:50:47,1:50:54
the constrained graph the graph which encodes the alignments of the length that we care about i

1:50:54,1:51:02
i have my emissions graph which is the z graph i don't need to do anything to it i compute their forward score this is a

1:51:02,1:51:08
function in gtn it's an operation i compute the loss which is the difference of the

1:51:08,1:51:14
two forward scores essentially and then in gtn what

1:51:14,1:51:20
what we can also do is well we can we can clear the gradient stored on the graph and then

1:51:20,1:51:26
automatically differentiate through all the operations we've just performed

1:51:26,1:51:31
and so when i call backward on the loss what happens under the hood is this

1:51:31,1:51:37
chain of back propagation computing gradients through operations

1:51:37,1:51:43
to the very kind of leafs of the of the tree that we constructed

1:51:43,1:51:51
and then that's it i can return my loss i can turn the ingredients and do it and use them for as i please

1:51:51,1:51:58
so this is the full loss function as you can see not a lot of code the main code was

1:51:58,1:52:03
constructing the graphs themselves and the loss function itself is actually very

1:52:03,1:52:11
generic in fact this loss function if i wanted to then make ctc from it

1:52:11,1:52:16
well what has changed

1:52:16,1:52:23
actually nothing changed it's the same code exactly the only difference

1:52:23,1:52:31
is how i specify the target aligned graph instead of using that simple structure

1:52:31,1:52:37
i used the graph that i showed you earlier which encodes the fact that there can be blanks and those blanks can

1:52:37,1:52:42
be optional but the loss function the way i write the code to compute the loss is

1:52:42,1:52:48
identical this is why it's so this is one of the big benefits of

1:52:48,1:52:57
of operating on graphs makes it really easy to try different algorithms

1:52:57,1:53:02
so the other thing i want to point out before i conclude is this code is really meant to

1:53:02,1:53:07
give you a flavor of how things work in gtn and it's in it's there's parallels

1:53:07,1:53:14
between this framework in something like pi torch right so you know if you were if you if you weren't if you

1:53:14,1:53:20
didn't know that this was gtn you might think this was pi torch like it's computing a loss

1:53:20,1:53:28
on some data structures it's computing gradients it's calling backward to do automatic differentiation

1:53:28,1:53:35
the only difference is the data structure and of course the operations that we can use on the data structure but otherwise it's very synonymous

1:53:35,1:53:41
so you know it's quite there's quite a lot of parallels between operating on graphs and operating on

1:53:41,1:53:48
sensors so that's all i have on gtns

1:53:48,1:53:53
and everything thank you all for listening and for inviting me for

1:53:53,1:53:59
the lecture today that was great oni i think this lesson was absolutely

1:53:59,1:54:05
magnificent thank you yeah thank you so much honey for

1:54:05,1:54:10
the lecture my pleasure there were quite a few questions that we answered

1:54:10,1:54:17
in the chat while you were talking great good thanks for doing that yeah so

1:54:17,1:54:23
happy to share slides or anything like that and Iâ€™ve included some kind of references here for further reading for

1:54:23,1:54:28
people that are interested in learning more okay and if people are

1:54:28,1:54:33
if students are interested in communicating and writing some of you some something to you

1:54:33,1:54:41
how can they get in contact in case right so my email is on the first slide fb.com

1:54:41,1:54:46
but if you google my name you know it'll lead to my website where my email is

1:54:46,1:54:52
also available okay that's great so again thanks again for being

1:54:52,1:55:03
with us and then everyone have a nice rest of the day and rest of the week okay take care everyone
